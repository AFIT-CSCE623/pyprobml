{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "# Use of this source code is governed by an MIT-style\n",
    "# license that can be found in the LICENSE file or at\n",
    "# https://opensource.org/licenses/MIT.\n",
    "\n",
    "# Author(s): Kevin P. Murphy (murphyk@gmail.com) and Mahmoud Soliman (mjs@aucegypt.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://opensource.org/licenses/MIT\" target=\"_parent\"><img src=\"https://img.shields.io/github/license/probml/pyprobml\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/probml/pyprobml/blob/master/notebooks/figures//chapter7_figures.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloning the pyprobml repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/probml/pyprobml \n",
    "%cd pyprobml/scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing required software (This may take few minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt install octave  -qq > /dev/null\n",
    "!apt-get install liboctave-dev -qq > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 7.1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  (a) Central interval and (b) HPD region for a Beta(3,9) posterior. The CI is (0.06, 0.52) and the HPD is (0.04, 0.48). Adapted from Figure 3.6 of \\citep  Hoff09 .  \n",
    "Figure(s) generated by [betaHPD.m](https://github.com/probml/pmtk3/blob/master/demos/betaHPD.m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W betaHPD.m >> _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 7.2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  (a) Central interval and (b) HPD region for a hypothetical multimodal posterior. Adapted from Figure 2.2 of \\citep  Gelman04 .  \n",
    "Figure(s) generated by [postDensityIntervals.m](https://github.com/probml/pmtk3/blob/master/demos/postDensityIntervals.m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W postDensityIntervals.m >> _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 7.3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Computing the distribution of $z=y^2$, where $p(y)$ is uniform (left). The analytic result is shown in the middle, and the Monte Carlo approximation is shown on the right.  \n",
    "Figure(s) generated by [change_of_vars_demo1d.py](https://github.com/probml/pyprobml/blob/master/scripts/change_of_vars_demo1d.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./change_of_vars_demo1d.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 7.4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Kernel density estimate derived from a Monte Carlo approximation to $p(\\mu ,\\sigma ^2| \\mathcal  D  )$ where $ \\mathcal  D  = \\  y_n \\sim \\mathcal  N (\\mu =2,\\sigma =1): n=1:1000 \\ $ and we use a diffuse prior. The red star is the MLE.  \n",
    "Figure(s) generated by [gauss2d_pymc3.py](https://github.com/probml/pyprobml/blob/master/scripts/gauss2d_pymc3.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./gauss2d_pymc3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 7.5:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Updating a Beta prior with a Bernoulli likelihood with sufficient statistics $N_1=4,N_0=1$. (a) Beta(2,2) prior. (b) Uniform Beta(1,1) prior.  \n",
    "Figure(s) generated by [beta_binom_post_plot.py](https://github.com/probml/pyprobml/blob/master/scripts/beta_binom_post_plot.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./beta_binom_post_plot.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 7.7:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  (a) Posterior predictive distributions for 10 future trials after seeing $N_1=4$ heads and $N_0=1$ tails. (b) Plug-in approximation based on the same data. In both cases, we use a uniform prior.  \n",
    "Figure(s) generated by [beta_binom_post_pred_plot.py](https://github.com/probml/pyprobml/blob/master/scripts/beta_binom_post_pred_plot.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./beta_binom_post_pred_plot.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 7.8:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  A mixture of two Beta distributions.  \n",
    "Figure(s) generated by [mixBetaDemo.m](https://github.com/probml/pmtk3/blob/master/demos/mixBetaDemo.m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W mixBetaDemo.m >> _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 7.9:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  (a) The Dirichlet distribution when $K=3$ defines a distribution over the simplex, which can be represented by the triangular surface. Points on this surface satisfy $0 \\leq \\theta _k \\leq 1$ and $\\DOTSB \\sum@ \\slimits@ _ k=1 ^3 \\theta _k = 1$.  \n",
    "Figure(s) generated by [dirichlet_3d_triangle_plot.py](https://github.com/probml/pyprobml/blob/master/scripts/dirichlet_3d_triangle_plot.py) [dirichlet_3d_spiky_plot.py](https://github.com/probml/pyprobml/blob/master/scripts/dirichlet_3d_spiky_plot.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./dirichlet_3d_triangle_plot.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./dirichlet_3d_spiky_plot.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 7.10:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Samples from a 5-dimensional symmetric Dirichlet distribution for different parameter values. (a) $\\oset  \\smallsmile   \\boldsymbol  \\alpha    = (0.1,\\ldots ,0.1)$. This results in very sparse distributions, with many 0s. (b) $\\oset  \\smallsmile   \\boldsymbol  \\alpha    = (1,\\ldots ,1)$. This results in more uniform (and dense) distributions.  \n",
    "Figure(s) generated by [dirichlet_samples_plot.py](https://github.com/probml/pyprobml/blob/master/scripts/dirichlet_samples_plot.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./dirichlet_samples_plot.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 7.11:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Inferring the mean of a univariate Gaussian with known $\\sigma ^2$. (a) Using strong prior, $p(\\mu ) = \\mathcal  N (\\mu |0,1)$. (b) Using weak prior, $p(\\mu ) = \\mathcal  N (\\mu |0,5)$.  \n",
    "Figure(s) generated by [gaussInferParamsMean1d.m](https://github.com/probml/pmtk3/blob/master/demos/gaussInferParamsMean1d.m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W gaussInferParamsMean1d.m >> _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 7.12:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Sequential updating of the posterior for $\\sigma ^2$ starting from an uninformative prior. The data was generated from a Gaussian with known mean $\\mu =5$ and unknown variance $\\sigma ^2=10$.  \n",
    "Figure(s) generated by [gauss_seq_update_sigma_1d.py](https://github.com/probml/pyprobml/blob/master/scripts/gauss_seq_update_sigma_1d.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./gauss_seq_update_sigma_1d.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 7.13:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  The $NI\\chi ^2(\\mu ,\\sigma ^2|m,\\kappa ,\\nu ,\\sigma ^2)$ distribution. $m$ is the prior mean and $k$ is how strongly we believe this; $\\sigma ^2$ is the prior variance and $\\nu $ is how strongly we believe this. (a) $m=0,\\kappa =1,\\nu =1,\\sigma ^2=1$. Notice that the contour plot (underneath the surface) is shaped like a ``squashed egg''. (b) We increase the strength of our belief in the mean by setting $\\kappa =5$, so the distribution for $\\mu $ around $m=0$ becomes narrower.  \n",
    "Figure(s) generated by [nix_plots.py](https://github.com/probml/pyprobml/blob/master/scripts/nix_plots.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./nix_plots.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 7.14:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Illustration of Bayesian inference for the mean of a 2d Gaussian. (a) The data is generated from $\\mathbf  y _n \\sim \\mathcal  N (\\boldsymbol  \\mu  ,\\boldsymbol  \\Sigma  )$, where $\\boldsymbol  \\mu  =[0.5, 0.5]^ \\top  $ and $\\boldsymbol  \\Sigma  =0.1 [2, 1; 1, 1])$. (b) The prior is $p(\\boldsymbol  \\mu  ) = \\mathcal  N (\\boldsymbol  \\mu  |\\boldsymbol  0 ,0.1 \\mathbf  I _2)$. (c) We show the posterior after 10 data points have been observed.  \n",
    "Figure(s) generated by [gaussInferParamsMean2d.m](https://github.com/probml/pmtk3/blob/master/demos/gaussInferParamsMean2d.m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W gaussInferParamsMean2d.m >> _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 7.17:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Data and inferences for the hierarchical binomial model fit using HMC.  \n",
    "Figure(s) generated by [hbayes_binom_rats_pymc3.py](https://github.com/probml/pyprobml/blob/master/scripts/hbayes_binom_rats_pymc3.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./hbayes_binom_rats_pymc3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 7.19:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  8-schools dataset. (a) Raw data. Each row plots $y_j \\pm \\sigma _j$. Vertical line is the pooled estimate. (b) Posterior 95\\% credible intervals for $\\theta _j$. Vertical line is posterior mean $\\mathbb  E \\left [ \\mu | \\mathcal  D   \\right ]$.  \n",
    "Figure(s) generated by [schools8_pymc3.py](https://github.com/probml/pyprobml/blob/master/scripts/schools8_pymc3.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./schools8_pymc3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 7.20:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Marginal posterior density $p(\\tau | \\mathcal  D  )$ for the 8-schools dataset.  \n",
    "Figure(s) generated by [schools8_pymc3.py](https://github.com/probml/pyprobml/blob/master/scripts/schools8_pymc3.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./schools8_pymc3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 7.21:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Data and inferences for the hierarchical binomial model fit using empirical Bayes.  \n",
    "Figure(s) generated by [ebBinom.m](https://github.com/probml/pmtk3/blob/master/demos/ebBinom.m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W ebBinom.m >> _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 7.22:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Ilustration of Bayesian model selection for polynomial regression. (a-c) We fit polynomials of degrees 1, 2 and 3 fit to $N=5$ data points. The solid green curve is the true function, the dashed red curve is the prediction (dotted blue lines represent $\\pm \\sigma $ around the mean). (d) We plot the posterior over models, $p(m| \\mathcal  D  )$, assuming a uniform prior $p(m) \\propto 1$. Adapted from a figure by Zoubin Ghahramani.  \n",
    "Figure(s) generated by [linreg_eb_modelsel_vs_n.py](https://github.com/probml/pyprobml/blob/master/scripts/linreg_eb_modelsel_vs_n.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./linreg_eb_modelsel_vs_n.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 7.23:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Same as \\cref  fig:linregEbModelSelVsN5  except now $N=30$.  \n",
    "Figure(s) generated by [linreg_eb_modelsel_vs_n.py](https://github.com/probml/pyprobml/blob/master/scripts/linreg_eb_modelsel_vs_n.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./linreg_eb_modelsel_vs_n.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 7.25:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  (a) Log marginal likelihood vs number of heads for the coin tossing example. (b) BIC approximation. (The vertical scale is arbitrary, since we are holding $N$ fixed.)  \n",
    "Figure(s) generated by [coins_model_sel_demo.py](https://github.com/probml/pyprobml/blob/master/scripts/coins_model_sel_demo.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./coins_model_sel_demo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 7.26:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Monte Carlo approximation to $p(\\delta | \\mathcal  D  )$. We use kernel density estimation (\\cref  sec:KDE ) to get a smooth plot. The vertical lines represent a ROPE of $[-0.1, 0.1]$.  \n",
    "Figure(s) generated by [amazonSellerDemo.m](https://github.com/probml/pmtk3/blob/master/demos/amazonSellerDemo.m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W amazonSellerDemo.m >> _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 7.27:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  (a) Histogram of Newcomb's data. (b) Histograms of data sampled from Gaussian model. (c) Histogram of test statistic on data sampled from the model, which represents $p(\\tau (\\cc@accent  \"707E   \\mathcal  D   ^s)| \\mathcal  D  )$, where $\\tau ( \\mathcal  D  )=\\qopname m min \\ y \\in  \\mathcal  D  \\ $. The vertical line is the test statistic on the true data, $\\tau ( \\mathcal  D  )$. (d) Same as (c) except $\\tau ( \\mathcal  D  ) = \\mathbb  V \\  y \\in  \\mathcal  D  \\ $.  \n",
    "Figure(s) generated by [newcombPlugin.m](https://github.com/probml/pmtk3/blob/master/demos/newcombPlugin.m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W newcombPlugin.m >> _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 7.28:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Approximating the posterior of a beta-Bernoulli model. (a) Grid approximation using 20 grid points. (b) Laplace approximation.  \n",
    "Figure(s) generated by [beta_binom_approx_post_pymc3.py](https://github.com/probml/pyprobml/blob/master/scripts/beta_binom_approx_post_pymc3.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./beta_binom_approx_post_pymc3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 7.29:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Approximating the posterior of a beta-Bernoulli model using Gaussian approximation to the logits, $q(\\alpha ) = \\mathcal  N (\\mu ,\\sigma )$, where $\\alpha =\\mathrm  logit (\\theta )$. (a) Estimate of variational mean over time. (b) Estimate of variational standard deviation over time. (c) ELBO over time. (d) Kernel density estimate of the original parameter $\\theta \\in [0,1]$ derived from samples from the variational posterior.  \n",
    "Figure(s) generated by [beta_binom_approx_post_pymc3.py](https://github.com/probml/pyprobml/blob/master/scripts/beta_binom_approx_post_pymc3.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./beta_binom_approx_post_pymc3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 7.30:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Approximating the posterior of a beta-Bernoulli model using MCMC. (a) Kernel density estimate derived from samples from 4 independent chains. (b) Trace plot of the chains as they generate posterior samples.  \n",
    "Figure(s) generated by [beta_binom_approx_post_pymc3.py](https://github.com/probml/pyprobml/blob/master/scripts/beta_binom_approx_post_pymc3.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./beta_binom_approx_post_pymc3.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copyright and License"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook was authored by Kevin P. Murphy (murphyk@gmail.com) and Mahmoud Soliman (mjs@aucegypt.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![GitHub](https://img.shields.io/github/license/probml/pyprobml)](https://github.com/probml/pml-book/blob/main/LICENSE/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/probml/pyprobml/blob/master/notebooks/figures//chapter17_figures.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloning the pyprobml repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/probml/pyprobml \n",
    "%cd pyprobml/scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 17.1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Interpolating a function from noise-free data using a smoothness prior with prior precision $\\lambda $. Blue crosses are observations. Red line is posterior mean, thin black lines are posterior samples. Shaded gray area is the pointwise 95\\% marginal credible interval for $f(x_j)$, i.e., $\\mu _j \\pm 2 \\sqrt  \\Sigma _ 1|2, jj  $. (a) $\\lambda =1$. (b) $\\lambda =0.1$. Adapted from Figure 7.1 of \\citep  Calvetti07 . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W gaussInterpDemoStable.m >> _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 17.2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Function samples from a GP with an ARD kernel. (a) $\\ell _1=\\ell _2=1$. Both dimensions contribute to the response. (b) $\\ell _1=1$, $\\ell _2=5$. The second dimension is essentially ignored. Adapted from Figure 5.1 of \\citep  Rasmussen06 . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W gprDemoArd.m >> _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 17.3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Functions sampled from a GP with a Matern kernel. (a) $\\nu =5/2$. (b) $\\nu =1/2$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W gpKernelPlot.m >> _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 17.4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Functions sampled from a GP using various stationary periodic kernels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W gpKernelPlot.m >> _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 17.8:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Left: some functions sampled from a GP prior with squared exponential kernel. Right: some samples from a GP posterior, after conditioning on 5 noise-free observations. The shaded area represents $\\mathbb  E \\left [ f(\\mathbf  x ) \\right ] \\pm 2 \\mathrm  std \\left [ f(\\mathbf  x ) \\right ]$. Adapted from Figure 2.2 of \\citep  Rasmussen06 . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W gprDemoNoiseFree.m >> _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 17.9:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Some 1d GPs with SE kernels but different hyper-parameters fit to 20 noisy observations. The kernel has the form in \\cref  eqn:SE1 . The hyper-parameters $(\\ell ,\\sigma _f,\\sigma _y)$ are as follows: (a) (1,1,0.1) (b) (0.3, 1.08, 0.00005), (c) (3.0, 1.16, 0.89). Adapted from Figure 2.5 of \\citep  Rasmussen06 . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W gprDemoChangeHparams.m >> _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 17.10:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Illustration of local minima in the marginal likelihood surface. (a) We plot the log marginal likelihood vs $\\sigma _y^2$ and $\\ell $, for fixed $\\sigma _f^2=1$, using the 7 data points shown in panels b and c. (b) The function corresponding to the lower left local minimum, $(\\ell ,\\sigma ^2_n) \\approx (1,0.2)$. This is quite ``wiggly'' and has low noise. (c) The function corresponding to the top right local minimum, $(\\ell ,\\sigma ^2_n) \\approx (10,0.8)$. This is quite smooth and has high noise. The data was generated using $(\\ell ,\\sigma ^2_n)=(1,0.1)$. From Figure 5.5 of \\citep  Rasmussen06 . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W gprDemoMarglik.m >> _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 17.11:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  GP classifier for a binary classification problem on Iris flowers (setosa vs versicolor) using a single input feature (sepal length). The fat vertical line is the credible interval for the decision boundary. (a) SE kernel. (b) SE plus linear kernel. Adapted from Figures 7.11--7.12 of \\citep  Martin2018 . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./gp_classify_iris_1d_pymc3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 17.12:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  (a) Fictitious ``space flu'' binary classification problem. (b) Fit from a GP with SE kernel. Adapted from Figures 7.13--7.14 of \\citep  Martin2018 . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./gp_classify_spaceflu_1d_pymc3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 17.16:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Illustration of the benefits of scaling the input features before computing a max margin classifier. Adapted from Figure 5.2 of \\citep  Geron2019 . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./svm_classifer_feature_scaling.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 17.19:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  SVM classifier with RBF kernel with precision $\\gamma $ and regularizer $C$ applied to two moons data. Adapted from Figure 5.9 of \\citep  Geron2019 . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./svm_classifer_2d.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 17.20:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  (a) A cross validation estimate of the 0-1 error for an SVM classifier with RBF kernel with different precisions $\\gamma =1/(2\\sigma ^2)$ and different regularizer $\\lambda =1/C$, applied to a synthetic data set drawn from a mixture of 2 Gaussians. (b) A slice through this surface for $\\gamma =5$ The red dotted line is the Bayes optimal error, computed using Bayes rule applied to the model used to generate the data. Adapted from Figure 12.6 of \\citep  HastieBook . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W svmCgammaDemo.m >> _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 17.21:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  (a) Illustration of $\\ell _2$, Huber and $\\epsilon $-insensitive loss functions, where $\\epsilon =1.5$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W huberLossPlot.m >> _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 17.22:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Illustration of support vector regression. Adapted from Figure 5.11 of \\citep  Geron2019 . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./svm_regression_1d.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 17.23:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Example of non-linear binary classification using an RBF kernel with bandwidth $\\sigma =0.3$. (a) L2VM with $\\lambda =5$. (b) L1VM with $\\lambda =1$. (c) RVM. (d) SVM with $C=1/\\lambda $ chosen by cross validation. Black circles denote the support vectors. 178 out of the 200 points are chosen as SVs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W kernelBinaryClassifDemo.m >> _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 17.24:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Example of kernel based regression on the noisy sinc function using an RBF kernel with bandwidth $\\sigma =0.3$. (a) L2VM with $\\lambda =0.5$. (b) L1VM with $\\lambda =0.5$. (c) RVM. (d) SVM regression with $C=1/\\lambda $. and $\\epsilon =0.1$ (the default for SVMlight). Red circles denote the retained training exemplars. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W svmRegrDemo.m >> _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 17.25:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Coefficient vectors of length $N=100$ for the models in \\cref  fig:kernelRegrDemoData . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W svmRegrDemo.m >> _"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}

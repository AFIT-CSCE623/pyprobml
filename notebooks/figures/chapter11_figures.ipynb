{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copyright and License"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook was authored by Kevin P. Murphy (murphyk@gmail.com) and Mahmoud Soliman (mjs@aucegypt.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![GitHub](https://img.shields.io/github/license/probml/pyprobml)](https://github.com/probml/pml-book/blob/main/LICENSE/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/probml/pyprobml/blob/master/notebooks/figures//chapter11_figures.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloning the pyprobml repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/probml/pyprobml \n",
    "%cd pyprobml/scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 11.1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Polynomial of degrees 1 and 2 fit to 21 datapoints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./linreg_poly_vs_degree.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 11.2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  (a) Contours of the RSS error surface for the example in \\cref  fig:linregPolyDegree1 . The blue cross represents the MLE. (b) Corresponding surface plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./linreg_contours_sse_plot.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 11.4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Regression coefficients over time for the 1d model in \\cref  fig:linregPoly2 (a). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W linregOnlineDemo.m >> _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 11.5:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Residual plot for polynomial regression of degree 1 and 2 for the functions in \\cref  fig:linregPoly2 (a-b). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./linreg_poly_vs_degree.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 11.6:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Fit vs actual plots for polynomial regression of degree 1 and 2 for the functions in \\cref  fig:linregPoly2 (a-b). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./linreg_poly_vs_degree.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 11.7:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  (a-c) Ridge regression applied to a degree 14 polynomial fit to 21 datapoints. (d) MSE vs strength of regularizer. The degree of regularization increases from left to right, so model complexity decreases from left to right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./linreg_poly_ridge.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 11.8:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Geometry of ridge regression. The likelihood is shown as an ellipse, and the prior is shown as a circle centered on the origin. Adapted from Figure 3.15 of \\citep  BishopBook . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W geomRidge.m >> _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 11.9:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  (a) Illustration of robust linear regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W linregRobustDemoCombined.m >> _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W huberLossPlot.m >> _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 11.12:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  (a) Profiles of ridge coefficients for the prostate cancer example vs bound $B$ on $\\ell _2$ norm of $\\mathbf  w $, so small $B$ (large $\\lambda $) is on the left. The vertical line is the value chosen by 5-fold CV using the 1 standard error rule. Adapted from Figure 3.8 of \\citep  HastieBook . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W ridgePathProstate.m >> _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W lassoPathProstate.m >> _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 11.14:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  (a) Boxplot displaying (absolute value of) prediction errors on the prostate cancer test set for different regression methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./prostateComparison.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W sparseSensingDemo.m >> _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 11.15:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Illustration of group lasso where the original signal is piecewise Gaussian. (a) Original signal. (b) Vanilla lasso estimate. (c) Group lasso estimate using a $\\ell _2$ norm on the blocks. (d) Group lasso estimate using an $\\ell _ \\infty  $ norm on the blocks. Adapted from Figures 3-4 of \\citep  Wright09 . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W groupLassoDemo.m >> _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 11.17:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Sequential Bayesian inference of the parameters of a linear regression model $p(y|\\mathbf  x ) = \\mathcal  N (y | w_0 + w_1 x_1, \\sigma ^2)$. Left column: likelihood function for current data point. Middle column: posterior given first $N$ data points, $p(w_0,w_1|\\mathbf  x _ 1:N ,y_ 1:N ,\\sigma ^2)$. Right column: samples from the current posterior predictive distribution. Row 1: prior distribution ($N=0$). Row 2: after 1 data point. Row 3: after 2 data points. Row 4: after 100 data points. The white cross in columns 1 and 2 represents the true parameter value; we see that the mode of the posterior rapidly converges to this point. The blue circles in column 3 are the observed data points. Adapted from Figure 3.7 of \\citep  BishopBook . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./linreg_2d_bayes_demo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 11.18:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Posterior samples of $p(w_0,w_1| \\mathcal  D  )$ for 1d linear regression model $p(y|x,\\boldsymbol  \\theta  )=\\mathcal  N (y|w_0 + w_1 x, \\sigma ^2)$ with a Gaussian prior. (a) Original data. (b) Centered data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./linreg_2d_bayes_centering_pymc3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 11.19:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  (a) Plugin approximation to predictive density (we plug in the MLE of the parameters) when fitting a second degree polynomial to some 1d data. (b) Posterior predictive density, obtained by integrating out the parameters. Black curve is posterior mean, error bars are 2 standard deviations of the posterior predictive density. (c) 10 samples from the plugin approximation to posterior predictive distribution. (d) 10 samples from the true posterior predictive distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./linreg_post_pred_plot.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 11.26:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  (a) A dynamic generalization of linear regression. (b) Illustration of the recursive least squares algorithm applied to the model $p(y|\\mathbf  x ,\\boldsymbol  \\theta  ) = \\mathcal  N (y|w_0 + w_1 x, \\sigma ^2)$. We plot the marginal posterior of $w_0$ and $w_1$ vs number of data points. (Error bars represent $\\mathbb  E \\left [ w_j|y_ 1:t  \\right ] \\pm \\sqrt  \\mathbb  V \\left [  w_j|y_ 1:t  \\right ] $.) After seeing all the data, we converge to the offline ML (least squares) solution, represented by the horizontal lines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W linregOnlineDemoKalman.m >> _"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}

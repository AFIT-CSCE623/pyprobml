{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "# Use of this source code is governed by an MIT-style\n",
    "# license that can be found in the LICENSE file or at\n",
    "# https://opensource.org/licenses/MIT.\n",
    "\n",
    "# Author(s): Kevin P. Murphy (murphyk@gmail.com) and Mahmoud Soliman (mjs@aucegypt.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://opensource.org/licenses/MIT\" target=\"_parent\"><img src=\"https://img.shields.io/github/license/probml/pyprobml\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/probml/pyprobml/blob/master/notebooks/figures//chapter19_figures.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloning the pyprobml repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/probml/pyprobml \n",
    "%cd pyprobml/scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "\n",
    "DISCLAIMER = 'WARNING : Editing in VM - changes lost after reboot!!'\n",
    "from google.colab import files\n",
    "\n",
    "def interactive_script(script, i=True):\n",
    "  if i:\n",
    "    s = open(script).read()\n",
    "    if not s.split('\\n', 1)[0]==\"## \"+DISCLAIMER:\n",
    "      open(script, 'w').write(\n",
    "          f'## {DISCLAIMER}\\n' + '#' * (len(DISCLAIMER) + 3) + '\\n\\n' + s)\n",
    "    files.view(script)\n",
    "    %run $script\n",
    "  else:\n",
    "      %run $script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 19.1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Illustration of random crops, zooms and rotations of some cat images. From \\cite  kerasBook . Used with kind permission of Francois Chollet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 19.2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Illustration of transfer learning from dataset $ \\mathcal  D  _p$ to $ \\mathcal  D  _q$ using a neural network, in which the feature extractor is shared, but the final layer is domain specific. The parameters $\\boldsymbol  \\theta  _1$ are first trained on $ \\mathcal  D  _p$, and then optionally fine-tuned on $ \\mathcal  D  _q$. Thus the information in $ \\mathcal  D  _p$ is used to help the model work well on $ \\mathcal  D  _q$, but not vice versa. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 19.3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  (a) Context encoder for self-supervised learning. From <a href='#Pathak2016'>[Dee+16]</a> . Used with kind permission of Deepak Pathak. (b) Some other proxy tasks for self-supervised learning. From <a href='#LeCunSSL2018'>[LeC18]</a> . Used with kind permission of Yann LeCun. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 19.4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  (a) Illustration of SimCLR training. $\\mathcal  T $ is a set of stochastic semantics-preserving transformations (data augmentations). (b-c) Illustration of the benefit of random crops. Solid rectangles represent the original image, dashed rectangles are random crops. On the left, the model is forced to predict the local view A from the global view B (and vice versa). On the right, the model is forced to predict the appearance of adjacent views (C,D). From Figures 2--3 of <a href='#chen2020simple'>[Tin+20]</a> . Used with kind permission of Ting Chen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 19.5:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Visualization of SimCLR training. Each input image in the minibatch is randomly modified in two different ways (using cropping (followed by resize), flipping, and color distortion), and then fed into a Siamese network. The embeddings (final layer) for each pair derived from the same image is forced to be close, whereas the embeddings for all other pairs are forced to be far. From   https://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html . Used with kind permission of Ting Chen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 19.6:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Graphical model corresponding to MAML. Left: generative model. Right: During meta-training, each of the task parameters $\\boldsymbol  \\theta  _j$'s are updated using their local datasets. The indices $j$ are over tasks (meta datasets), and $i$ are over instances within each task. Solid shaded nodes are always observed; semi-shaded (striped) nodes are only observed during meta training time (i.e., not at test time). From Figure 1 of <a href='#Finn2018'>[CKS18]</a> . Used with kind permission of Chelsea Finn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 19.7:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Illustration of meta-learning for few-shot learning. Here, each task is a 3-way-2-shot classification problem because each training task contains a support set with three classes, each with two examples. From   https://www.borealisai.com/en/blog/tutorial-2-few-shot-learning-and-meta-learning-i . Copyright (2019) Borealis AI. Used with kind permission of Simon Prince and April Cooper. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 19.8:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Illustration of a matching network for one-shot learning. From Figure 1 of <a href='#Vinyals2016'>[Ori+16]</a> . Used with kind permission of Oriol Vinyals. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 19.9:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Illustration of the cosine similarity between a query vector $\\mathbf  q $ and two document vectors $\\mathbf  d _1$ and $\\mathbf  d _2$. Since angle $\\alpha $ is less than angle $\\theta $, we see that the query is more similar to document 1. From   https://en.wikipedia.org/wiki/Vector_space_model . Used with kind permission of Wikipedia author Riclas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 19.10:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Illustration of word2vec model with window size $H=2$. (a) CBOW version. (b) Skip-gram version. . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 19.11:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Visualization of arithmetic operations in word2vec embedding space. From   https://www.tensorflow.org/tutorials/representation/word2vec . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 19.12:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Illustration of ELMo bidrectional language model. Here $y_t=x_ t+1 $ when acting as the target for the forwards LSTM, and $y_t = x_ t-1 $ for the backwards LSTM. (We add \\text  \\em  bos  \\xspace and \\text  \\em  eos  \\xspace sentinels to handle the edge cases.) From Weng2019LM. Used with kind permission of Lilian Weng. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 19.13:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Illustration of (a) GPT and (b) BERT. $E_t$ is the embedding vector for the input token at location $t$, and $T_t$ is the output target to be predicted. From Figure 3 of <a href='#bert'>[Jac+19]</a> . Used with kind permission of Ming-Wei Chang. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 19.14:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Illustration of how BERT can be used for different kinds of supervised NLP tasks. (a) Sentence-pair classification (e.g., entailment). (b) Single sentence classification (e.g., sentiment). (c) Sentence pair tagging (e.g., question answering). (d) Single sentence tagging (e.g., named entity recognition, where the tags are ``outside'', ``begin-person'', ``inside-person'', ``begin-place'', ``inside-place'', etc). From Figure 4 of <a href='#bert'>[Jac+19]</a> . Used with kind permission of Ming-Wei Chang. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 19.15:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Question-answer pairs for a sample passage in the SQuAD dataset. Each of the answers is a segment of text from the passage. This can be solved using sentence pair tagging. The input is the paragraph text T and the question Q. The output is a tagging of the relevant words in T that answer the question in Q. From Figure 1 of <a href='#squad'>[Pra+16]</a> . Used with kind permission of Percy Liang. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 19.16:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Illustration of how the T5 model (``Text-to-text Transfer Transformer'') can be used to perform multiple NLP tasks, such as translating English to German; determining if a sentence is linguistic valid or not ( \\bf CoLA  stands for ``Corpus of Linguistic Acceptability''); determining the degree of semantic similarity ( \\bf STSB  stands for ``Semantic Textual Similarity Benchmark''); and abstractive summarization. From Figure 1 of <a href='#T5'>[Col+19]</a> . Used with kind permission of Colin Raffel. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 19.17:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Illustration of the benefits of semi-supervised learning for a binary classification problem. Labeled points from each class are shown as black and white circles respectively. (a) Decision boundary we might learn given only unlabeled data. (b) Decision boundary we might learn if we also had a lot of unlabeled data points, shown as smaller grey circles. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 19.18:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Comparison of the entropy minimization, self-training, and ``sharpened'' entropy minimization loss functions for a binary classification problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 19.19:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Visualization demonstrating how entropy minimization enforces the cluster assumption. The classifier assigns a higher probability to class 1 (black dots) or 2 (white dots) in red or blue regions respectively. The predicted class probabilities for one particular unlabeled datapoint is shown in the bar plot. In (a), the decision boundary passes through high-density regions of data, so the classifier is forced to output high-entropy predictions. In (b), the classifier avoids high-density regions and is able to assign low-entropy predictions to most of the unlabeled data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 19.20:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Comparison of the squared error and KL divergence lossses for a consistency regularization. This visualization is for a binary classification problem where it is assumed that the model's output for the unperturbed input is 1. The figure plots the loss incurred for a particular value of the logit (i.e.\\ the pre-activation fed into the output sigmoid nonlinearity) for the perturbed input. As the logit grows towards infinity, the model predicts a class label of 1 (in agreement with the prediction for the unperturbed input); as it grows towards negative infinity, the model predictions class 0. The squared error loss saturates (and has zero gradients) when the model predicts one class or the other with high probability, but the KL divergence grows without bound as the model predicts class 0 with more and more confidence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 19.21:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Diagram of the semi-supervised GAN framework. The discriminator is trained to output the class of labeled datapoints (red), a ``fake'' label for outputs from the generator (yellow), and any label for unlabeled data (green). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 19.22:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Combinng self-supervised learning on unlabeled data (left), supervised fine-tuning (middle), and self-training on pseudo-labeled data (right). From Figure 3 of <a href='#Chen2020nips'>[Tin+20]</a> . Used with kind permission of Ting Chen "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    " <a name='Finn2018'>[CKS18]</a> F. Chelsea, X. Kelvin and L. Sergey. \"Probabilistic Model-Agnostic Meta-Learning\". (2018). \n",
    "\n",
    "<a name='T5'>[Col+19]</a> R. Colin, S. Noam, R. Adam, L. LeeKatherine, N. Sharan, M. Michael, Z. ZhouYanqi, L. Wei and L. PeterJ. \"Exploring the Limits of Transfer Learning with a UnifiedText-to-Text Transformer\". abs/1910.10683 (2019). arXiv: 1910.10683 \n",
    "\n",
    "<a name='Pathak2016'>[Dee+16]</a> P. Deepak, K. Philipp, D. Jeff, D. Trevor and E. AlexeiA. \"Context Encoders: Feature Learning by Inpainting\". (2016). \n",
    "\n",
    "<a name='bert'>[Jac+19]</a> D. Jacob, C. Ming-Wei, L. Kenton and T. ToutanovaKristina. \"BERT: Pre-training of Deep Bidirectional Transformers forLanguage Understanding\". (2019). \n",
    "\n",
    "<a name='LeCunSSL2018'>[LeC18]</a> Y. LeCun \"Self-supervised learning: could machines learn like humans?\". (2018). \n",
    "\n",
    "<a name='Vinyals2016'>[Ori+16]</a> V. Oriol, B. Charles, L. Timothy, K. Koray and W. Daan. \"Matching Networks for One Shot Learning\". (2016). \n",
    "\n",
    "<a name='squad'>[Pra+16]</a> R. Pranav, Z. Jian, L. Konstantin and L. Percy. \"SQuAD: 100,000+ Questions for Machine Comprehension of Text\". (2016). \n",
    "\n",
    "<a name='Chen2020nips'>[Tin+20]</a> C. Ting, K. Simon, S. Kevin, N. NorouziMohammad and H. Geoffrey. \"Big Self-Supervised Models are Strong Semi-SupervisedLearners\". (2020). \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

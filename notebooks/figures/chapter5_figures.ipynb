{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "# Use of this source code is governed by an MIT-style\n",
    "# license that can be found in the LICENSE file or at\n",
    "# https://opensource.org/licenses/MIT.\n",
    "\n",
    "# Author(s): Kevin P. Murphy (murphyk@gmail.com) and Mahmoud Soliman (mjs@aucegypt.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://opensource.org/licenses/MIT\" target=\"_parent\"><img src=\"https://img.shields.io/github/license/probml/pyprobml\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/probml/pyprobml/blob/master/notebooks/figures//chapter5_figures.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloning the pyprobml repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/probml/pyprobml \n",
    "%cd pyprobml/scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing required software (This may take few minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt install octave  -qq > /dev/null\n",
    "!apt-get install liboctave-dev -qq > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 5.4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Steepest descent on a simple convex function, starting from $(0,0)$, for 20 steps, using a fixed step size. The global minimum is at $(1,1)$. (a) $\\eta =0.1$. (b) $\\eta =0.6$.  \n",
    "Figure(s) generated by [steepestDescentDemo.m](https://github.com/probml/pmtk3/blob/master/demos/steepestDescentDemo.m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W steepestDescentDemo.m >> _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 5.6:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  (a) Steepest descent on the same function as \\cref  fig:aokiFixed , starting from $(0,0)$, using line search.  \n",
    "Figure(s) generated by [steepestDescentDemo.m](https://github.com/probml/pmtk3/blob/master/demos/steepestDescentDemo.m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W steepestDescentDemo.m >> _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 5.8:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Illustration of Newton's method for minimizing a 1d function. (a) The solid curve is the function $\\mathcal  L (x)$. The dotted line $\\mathcal  L _ \\mathrm  quad  (\\theta )$ is its second order approximation at $\\theta _t$. The Newton step $d_t$ is what must be added to $\\theta _t$ to get to the minimum of $\\mathcal  L _ \\mathrm  quad  (\\theta )$. Adapted from Figure 13.4 of \\citep  Vandenberghe06 .  \n",
    "Figure(s) generated by [newtonsMethodMinQuad.m](https://github.com/probml/pmtk3/blob/master/demos/newtonsMethodMinQuad.m) [newtonsMethodNonConvex.m](https://github.com/probml/pmtk3/blob/master/demos/newtonsMethodNonConvex.m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W newtonsMethodMinQuad.m >> _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W newtonsMethodNonConvex.m >> _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 5.11:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Illustration of the benefits of natural gradient vs steepest descent on a 2D problem. (a) Trajectories of the two methods in parameter space (red = steepest descent, blue = NG). They both start at $(1,-1)$, bottom right location. (b) Objective vs number of iterations. (c) Gradient field in the $\\boldsymbol  \\theta  $ parameter space. (d) Gradient field in the whitened $\\boldsymbol  \\phi  = \\mathbf  F ^ \\frac  1  2   \\boldsymbol  \\theta  $ parameter space used by NG.  \n",
    "Figure(s) generated by [nat_grad_demo.py](https://github.com/probml/pyprobml/blob/master/scripts/nat_grad_demo.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./nat_grad_demo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 5.12:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Illustration of the LMS algorithm. Left: we start from $\\boldsymbol  \\theta  =(-0.5,2)$ and slowly converging to the least squares solution of $ \\boldsymbol  \\theta   =(1.45, 0.93)$ (red cross). Right: plot of objective function over time. Note that it does not decrease monotonically.  \n",
    "Figure(s) generated by [lms_demo.py](https://github.com/probml/pyprobml/blob/master/scripts/lms_demo.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./lms_demo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 5.16:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Illustration of a bound optimization algorithm. Adapted from Figure 9.14 of \\citep  BishopBook .  \n",
    "Figure(s) generated by [emLogLikelihoodMax.m](https://github.com/probml/pmtk3/blob/master/demos/emLogLikelihoodMax.m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W emLogLikelihoodMax.m >> _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 5.18:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Illustration of the EM for a GMM applied to the Old Faithful data for the first 6 steps. The degree of redness indicates the degree to which the point belongs to the red cluster, and similarly for blue; thus purple points have a roughly 50/50 split in their responsibilities to the two clusters. Adapted from \\citep  BishopBook  Figure 9.8.  \n",
    "Figure(s) generated by [mixGaussDemoFaithful.m](https://github.com/probml/pmtk3/blob/master/demos/mixGaussDemoFaithful.m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W mixGaussDemoFaithful.m >> _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 5.19:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  (a) Illustration of how singularities can arise in the likelihood function of GMMs. Here $K=2$, but the first mixture component is a narrow spike (with $\\sigma _1 \\approx 0$) centered on a single data point $x_1$. Adapted from Figure 9.7 of \\citep  BishopBook .  \n",
    "Figure(s) generated by [mixGaussSingularity.m](https://github.com/probml/pmtk3/blob/master/demos/mixGaussSingularity.m) [mixGaussMLvsMAP.m](https://github.com/probml/pmtk3/blob/master/demos/mixGaussMLvsMAP.m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W mixGaussSingularity.m >> _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W mixGaussMLvsMAP.m >> _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 5.20:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Left: $N=200$ data points sampled from a mixture of 2 Gaussians in 1d, with $\\pi _k=0.5$, $\\sigma _k=5$, $\\mu _1=-10$ and $\\mu _2=10$. Right: Likelihood surface $p( \\mathcal  D  |\\mu _1,\\mu _2)$, with all other parameters set to their true values. We see the two symmetric modes, reflecting the unidentifiability of the parameters.  \n",
    "Figure(s) generated by [mixGaussLikSurfaceDemo.m](https://github.com/probml/pmtk3/blob/master/demos/mixGaussLikSurfaceDemo.m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W mixGaussLikSurfaceDemo.m >> _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 5.21:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Illustration of data imputation. (a) Scatter plot of true values vs imputed values using true parameters. (b) Same as (a), but using parameters estimated with EM. We just show the first four variables, for brevity.  \n",
    "Figure(s) generated by [gaussImputationDemoEM.m](https://github.com/probml/pmtk3/blob/master/demos/gaussImputationDemoEM.m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W gaussImputationDemoEM.m >> _"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "# Use of this source code is governed by an MIT-style\n",
    "# license that can be found in the LICENSE file or at\n",
    "# https://opensource.org/licenses/MIT.\n",
    "\n",
    "# Author(s): Kevin P. Murphy (murphyk@gmail.com) and Mahmoud Soliman (mjs@aucegypt.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://opensource.org/licenses/MIT\" target=\"_parent\"><img src=\"https://img.shields.io/github/license/probml/pyprobml\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/probml/pyprobml/blob/master/notebooks/figures//chapter10_figures.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloning the pyprobml repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/probml/pyprobml \n",
    "%cd pyprobml/scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing required software (This may take few minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install octave  -qq > /dev/null\n",
    "!apt-get install liboctave-dev -qq > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "\n",
    "DISCLAIMER = 'WARNING : Editing in VM - changes lost after reboot!!'\n",
    "from google.colab import files\n",
    "\n",
    "def interactive_script(script, i=True):\n",
    "  if i:\n",
    "    s = open(script).read()\n",
    "    if not s.split('\\n', 1)[0]==\"## \"+DISCLAIMER:\n",
    "      open(script, 'w').write(\n",
    "          f'## {DISCLAIMER}\\n' + '#' * (len(DISCLAIMER) + 3) + '\\n\\n' + s)\n",
    "    files.view(script)\n",
    "    %run $script\n",
    "  else:\n",
    "      %run $script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 10.1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  (a) Visualization of a 2d plane in a 3d space with surface normal $\\mathbf  w $ going through point $\\mathbf  x _0=(x_0,y_0,z_0)$. See text for details. (b) Visualization of optimal linear decision boundary induced by logistic regression on a 2-class, 2-feature version of the iris dataset.  \n",
    "Figure(s) generated by [iris_logreg.py](https://github.com/probml/pyprobml/blob/master/scripts/iris_logreg.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_script(\"iris_logreg.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 10.2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Plots of $\\sigma (w_1 x_1 + w_2 x_2)$. Here $\\mathbf  w = (w_1,w_2)$ defines the normal to the decision boundary. Points to the right of this have $\\sigma (\\mathbf  w ^\\top \\mathbf  x )>0.5$, and points to the left have $\\sigma (\\mathbf  w ^\\top \\mathbf  x ) < 0.5$. Adapted from Figure 39.3 of <a href='#MacKay03'>[Mac03]</a> .  \n",
    "Figure(s) generated by [sigmoid_2d_plot.py](https://github.com/probml/pyprobml/blob/master/scripts/sigmoid_2d_plot.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_script(\"sigmoid_2d_plot.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 10.3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Illustration of how we can transform a quadratic decision boundary into a linear one by transforming the features from $\\mathbf  x =(x_1,x_2)$ to $\\boldsymbol  \\phi  (\\mathbf  x )=(x_1^2,x_2^2)$. Used with kind permission of Jean-Philippe Vert "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 10.4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Polynomial feature expansion applied to a two-class, two-dimensional logistic regression problem. (a) Degree $K=1$. (a) Degree $K=2$. (a) Degree $K=4$. (d) Train and test error vs degree.  \n",
    "Figure(s) generated by [logreg_poly_demo.py](https://github.com/probml/pyprobml/blob/master/scripts/logreg_poly_demo.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_script(\"logreg_poly_demo.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 10.5:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  NLL loss surface for binary logistic regression applied to Iris dataset with 1 feature and 1 bias term. The goal is to minimize the function.  \n",
    "Figure(s) generated by [iris_logreg_loss_surface.py](https://github.com/probml/pyprobml/blob/master/scripts/iris_logreg_loss_surface.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_script(\"iris_logreg_loss_surface.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 10.6:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Weight decay with variance $C$ applied to two-class, two-dimensional logistic regression problem with a degree 4 polynomial. (a) $C=1$. (a) $C=316$. (a) $C=100,000$. (d) Train and test error vs $C$.  \n",
    "Figure(s) generated by [logreg_poly_demo.py](https://github.com/probml/pyprobml/blob/master/scripts/logreg_poly_demo.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_script(\"logreg_poly_demo.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 10.7:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Example of 3-class logistic regression with 2d inputs. (a) Original features. (b) Quadratic features.  \n",
    "Figure(s) generated by [logreg_multiclass_demo.py](https://github.com/probml/pyprobml/blob/master/scripts/logreg_multiclass_demo.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_script(\"logreg_multiclass_demo.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 10.8:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  A simple example of a label hierarchy. Nodes within the same ellipse have a mutual exclusion relationship between them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 10.9:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  A flat and hierarchical softmax model $p(y|x)$, where $x$ are the input features (context) and $y$ is the output label. From https://www.quora.com/What-is-hierarchical-softmax "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 10.10:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Example of a term-document matrix, where raw counts have been replaced by their TF-IDF values (see \\cref  sec:tfidf ). Darker cells are larger values. From   https://bit.ly/2kByLQI . Used with kind permission of Christoph Carl Kling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 10.11:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  (a) Logistic regression on some data with outliers (denoted by x). Training points have been (vertically) jittered to avoid overlapping too much. Vertical line is the decision boundary, and its posterior credible interval. (b) Same as (a) but using robust model, with a mixture likelihood. Adapted from Figure 4.13 of <a href='#Martin2018'>[Mar18]</a> .  \n",
    "Figure(s) generated by [logreg_iris_bayes_robust_1d_pymc3.py](https://github.com/probml/pyprobml/blob/master/scripts/logreg_iris_bayes_robust_1d_pymc3.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_script(\"logreg_iris_bayes_robust_1d_pymc3.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 10.12:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  (a) Illustration of logistic and tempered logistic loss with $t_1=0.8$. (b) Illustration of sigmoid and tempered sigmoid transfer function with $t_2=2.0$. From   https://ai.googleblog.com/2019/08/bi-tempered-logistic-loss-for-training.html . Used with kind permission of Ehsan Amid. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 10.13:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Illustration of standard and bi-tempered logistic regression on data with label noise. From   https://ai.googleblog.com/2019/08/bi-tempered-logistic-loss-for-training.html . Used with kind permission of Ehsan Amid. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 10.14:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  (a) Illustration of the data. (b) Log-likelihood for a logistic regression model. The line is drawn from the origin in the direction of the MLE (which is at infinity). The numbers correspond to 4 points in parameter space, corresponding to the lines in (a). (c) Unnormalized log posterior (assuming vague spherical prior). (d) Laplace approximation to posterior. Adapted from a figure by Mark Girolami.  \n",
    "Figure(s) generated by [logregLaplaceGirolamiDemo.m](https://github.com/probml/pmtk3/blob/master/demos/logregLaplaceGirolamiDemo.m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W logregLaplaceGirolamiDemo.m >> _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 10.15:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Posterior predictive distribution for a logistic regression model in 2d. Top left: contours of $p(y=1|\\mathbf  x , \\mathbf  w  _ map )$. Top right: samples from the posterior predictive distribution. Bottom left: Averaging over these samples. Bottom right: moderated output (probit approximation). Adapted from a figure by Mark Girolami.  \n",
    "Figure(s) generated by [logregLaplaceGirolamiDemo.m](https://github.com/probml/pmtk3/blob/master/demos/logregLaplaceGirolamiDemo.m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W logregLaplaceGirolamiDemo.m >> _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 10.16:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Illustration of the posterior over the decision boundary for classifying iris flowers (setosa vs versicolor) using 2 input features. (a) 25 examples per class. Adapted from Figure 4.5 of <a href='#Martin2018'>[Mar18]</a> . (b) 5 examples of class 0, 45 examples of class 1. Adapted from Figure 4.8 of <a href='#Martin2018'>[Mar18]</a> .  \n",
    "Figure(s) generated by [logreg_iris_bayes_2d_pymc3.py](https://github.com/probml/pyprobml/blob/master/scripts/logreg_iris_bayes_2d_pymc3.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_script(\"logreg_iris_bayes_2d_pymc3.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 10.17:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Quadratic lower bounds on the sigmoid (logistic) function. In solid red, we plot $\\sigma (x)$ vs $x$. In dotted blue, we plot the lower bound $L(x, \\boldsymbol  \\xi  )$ vs $x$ for $\\boldsymbol  \\xi  =2.5$. (a) JJ bound. This is tight at $\\boldsymbol  \\xi  = \\pm 2.5$. (b) Bohning bound (\\cref  sec:bohningBinary . This is tight at $\\boldsymbol  \\xi  =2.5$.  \n",
    "Figure(s) generated by [sigmoidLowerBounds.m](https://github.com/probml/pmtk3/blob/master/demos/sigmoidLowerBounds.m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W sigmoidLowerBounds.m >> _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 10.18:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  A dynamic logistic regression model. $\\mathbf  w _t$ are the regression weights at time $t$, and $a_t = \\mathbf  w _t^\\top \\mathbf  x _t$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 10.19:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  (a) Data for logistic regression question. (b) Plot of $ w _k$ vs amount of correlation $c_k$ for three different estimators. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    " <a name='MacKay03'>[Mac03]</a> D. MacKay \"Information Theory, Inference, and Learning Algorithms\". (2003). \n",
    "\n",
    "<a name='Martin2018'>[Mar18]</a> O. Martin \"Bayesian analysis with Python\". (2018). \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

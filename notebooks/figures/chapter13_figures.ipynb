{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "# Use of this source code is governed by an MIT-style\n",
    "# license that can be found in the LICENSE file or at\n",
    "# https://opensource.org/licenses/MIT.\n",
    "\n",
    "# Author(s): Kevin P. Murphy (murphyk@gmail.com) and Mahmoud Soliman (mjs@aucegypt.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://opensource.org/licenses/MIT\" target=\"_parent\"><img src=\"https://img.shields.io/github/license/probml/pyprobml\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/probml/pyprobml/blob/master/notebooks/figures//chapter13_figures.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloning the pyprobml repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/probml/pyprobml \n",
    "%cd pyprobml/scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing required software (This may take few minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt install octave  -qq > /dev/null\n",
    "!apt-get install liboctave-dev -qq > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 13.1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  (a) Illustration of the fact that the XOR function is not linearly separable, but can be separated by the two layer model using Heaviside activation functions. Adapted from Figure 10.6 of \\citep  Geron2019 .  \n",
    "Figure(s) generated by [xor_heaviside.py](https://github.com/probml/pyprobml/blob/master/scripts/xor_heaviside.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./xor_heaviside.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 13.2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  (a) Illustration of how the sigmoid function is linear for inputs near 0, but saturates for large positive and negative inputs. Adapted from 11.1 of \\citep  Geron2019 .  \n",
    "Figure(s) generated by [activationFunPlot.py](https://github.com/probml/pyprobml/blob/master/scripts/activationFunPlot.py) [activationFunPlot.py](https://github.com/probml/pyprobml/blob/master/scripts/activationFunPlot.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./activationFunPlot.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./activationFunPlot.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 13.16:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Training loss vs learning rate for a small MLP fit to FashionMNIST using vanilla SGD. (Raw loss in blue, EWMA smoothed version in orange).  \n",
    "Figure(s) generated by [lrfinder.py](https://github.com/probml/pyprobml/blob/master/scripts/lrfinder.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./lrfinder.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 13.18:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  The effects of changing the hyperparameters on an MLP with one hidden layer. (a) Random functions sampled from a Gaussian prior with hyperparameters $\\alpha _ 1 =5$, $\\beta _1=1$, $\\alpha _2=1$, $\\beta _2=1$. (b) Increasing $\\alpha _ 1 $ by factor of 5. (c) Increasing $\\beta _ 1 $ by factor of 5. (d) Inreasing $\\alpha _ 2 $ by factor of 5.  \n",
    "Figure(s) generated by [mlpPriorsDemo2.m](https://github.com/probml/pmtk3/blob/master/demos/mlpPriorsDemo2.m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W mlpPriorsDemo2.m >> _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 13.19:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  (a) A deep but sparse neural network. The connections are pruned using $\\ell _1$ regularization. At each level, nodes numbered 0 are clamped to 1, so their outgoing weights correspond to the offset/bias terms. (b) Predictions made by the model on the training set.  \n",
    "Figure(s) generated by [sparseNnetDemo.m](https://github.com/probml/pmtk3/blob/master/demos/sparseNnetDemo.m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W sparseNnetDemo.m >> _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 13.21:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  (a) xor truth table. (b) Fitting a linear logistic regression classifier using degree 10 polynomial expansion. (c) Same model, but using an RBF kernel with centroids specified by the 4 black crosses.  \n",
    "Figure(s) generated by [logregXorDemo.m](https://github.com/probml/pmtk3/blob/master/demos/logregXorDemo.m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W logregXorDemo.m >> _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 13.22:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  RBF basis in 1d. Left column: fitted function. Middle column: basis functions evaluated on a grid. Right column: design matrix. Top to bottom we show different bandwidths: $\\tau =0.1$, $\\tau =0.5$, $\\tau =50$.  \n",
    "Figure(s) generated by [linregRbfDemo.m](https://github.com/probml/pmtk3/blob/master/demos/linregRbfDemo.m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W linregRbfDemo.m >> _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 13.23:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  (a) Some data from a one-to-many function. (b) The responsibilities of each expert for the input domain. (c) Prediction of each expert. (d) Overeall prediction. Mean is red cross, mode is black square. Adapted from Figures 5.20 and 5.21 of \\citep  BishopBook .  \n",
    "Figure(s) generated by [mixexpDemoOneToMany.m](https://github.com/probml/pmtk3/blob/master/demos/mixexpDemoOneToMany.m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!octave -W mixexpDemoOneToMany.m >> _"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

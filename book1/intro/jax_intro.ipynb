{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "jax_intro.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/probml/pyprobml/blob/master/book1/intro/jax_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4bE-S8yDALH"
      },
      "source": [
        "# Brief introduction to JAX \n",
        "\n",
        "murphyk@gmail.com, Last update: 2021-01-06.\n",
        "\n",
        "[JAX](https://github.com/google/jax) is a  version of NumPy that runs fast on CPU, GPU and TPU, by compiling down to XLA. It also has an excellent automatic differentiation library, extending the earlier [autograd](https://github.com/hips/autograd) package.\n",
        "\n",
        "The JAX interface is almost identical to NumPy (by design), but with some small differences, and additional features, some of which we explain below. (More detail can be find in the official documentation.)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCI0G3tfDFSs"
      },
      "source": [
        "# Standard Python libraries\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "np.set_printoptions(precision=3)\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL\n",
        "import imageio\n",
        "\n",
        "from IPython import display\n",
        "%matplotlib inline\n",
        "\n",
        "import sklearn\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9kAsUWYDIOk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ee8943a-2b6f-4f83-9622-31dba3ae01a9"
      },
      "source": [
        "\n",
        "# Load JAX\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "#import jax.numpy as np\n",
        "#import numpy as onp # original numpy\n",
        "from jax import grad, hessian, jit, vmap\n",
        "from jax import grad, hessian, jacfwd, jacrev, vmap, jit\n",
        "print(\"jax version {}\".format(jax.__version__))\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "jax version 0.2.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUOZdeYBKjWc"
      },
      "source": [
        "# Random number generation\n",
        "\n",
        "One of the biggest differences from NumPy is the way Jax treates pseudo random number generation (PRNG).\n",
        "This is because Jax does not maintain any global state, i.e., it is purely functional.\n",
        "This design \"provides reproducible results invariant to compilation boundaries and backends,\n",
        "while also maximizing performance by enabling vectorized generation and parallelization across random calls\"\n",
        "(to quote [the official page](https://github.com/google/jax#a-brief-tour)).\n",
        "                              \n",
        "Thus, whenever we do anything stochastic, we need to give it a fresh RNG key. We can do this by splitting the existing key into pieces. We can do this indefinitely, as shown below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcTYfznjKeHC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df552b41-14b4-42eb-c759-995ad63cc7be"
      },
      "source": [
        "import jax.random as random\n",
        "\n",
        "key = random.PRNGKey(0)\n",
        "print(random.normal(key, shape=(3,)))  # [ 1.81608593 -0.48262325  0.33988902]\n",
        "print(random.normal(key, shape=(3,)))  # [ 1.81608593 -0.48262325  0.33988902]  ## identical results\n",
        "\n",
        "# To make a new key, we split the current key into two pieces.\n",
        "key, subkey = random.split(key)\n",
        "print(random.normal(subkey, shape=(3,)))  # [ 1.1378783  -1.22095478 -0.59153646]\n",
        "\n",
        "# We can continue to split off new pieces from the global key.\n",
        "key, subkey = random.split(key)\n",
        "print(random.normal(subkey, shape=(3,)))  # [-0.06607265  0.16676566  1.17800343]\n",
        "\n",
        "# We can always use original numpy if we like (although this may interfere with the deterministic behavior of jax)\n",
        "np.random.seed(42)\n",
        "print(np.random.randn(3))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 1.816 -0.483  0.34 ]\n",
            "[ 1.816 -0.483  0.34 ]\n",
            "[ 1.138 -1.221 -0.592]\n",
            "[-0.066  0.167  1.178]\n",
            "[ 0.497 -0.138  0.648]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9saSIGTn661X"
      },
      "source": [
        "# GPU magic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXExOyfluzIj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "274311b7-3340-45ce-b598-67af9862e3c3"
      },
      "source": [
        "# Check if GPU is available\n",
        "!nvidia-smi\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Jan  4 22:36:19 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.27.04    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P8     9W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c4Y-hI3FR8f",
        "outputId": "830431c2-4c39-4c80-84dc-1ce7eac6beef"
      },
      "source": [
        "\n",
        "# Check if JAX is using GPU\n",
        "print(\"jax backend {}\".format(jax.lib.xla_bridge.get_backend().platform))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "jax backend gpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ri2ZN_C7ul5"
      },
      "source": [
        "Let's see how JAX can speed up things like matrix-matrix multiplication using a GPU.\n",
        "\n",
        "First the numpy/CPU version."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmSu22WZ7nRw",
        "outputId": "a6a1ce23-9604-4f52-fbe3-d621254f2f2e"
      },
      "source": [
        "# Standard CPU\n",
        "\n",
        "size = 1000\n",
        "x = np.random.normal(size=(size, size)).astype(np.float32)\n",
        "print(type(x))\n",
        "%timeit -o np.dot(x, x.T)\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "100 loops, best of 3: 17.7 ms per loop\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TimeitResult : 100 loops, best of 3: 17.7 ms per loop>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-y3VO5oc-PhO",
        "outputId": "4c8dfedb-c7e8-4c3f-9610-558cd5a96f98"
      },
      "source": [
        "res = _ # get result of last cell\n",
        "time_cpu = res.best\n",
        "print(time_cpu)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.01767426187999945\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJPBadZ88Hmi"
      },
      "source": [
        "Now the GPU version. We added that block_until_ready because JAX uses [asynchronous execution](https://jax.readthedocs.io/en/latest/async_dispatch.html) by default.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ptXbto48AWd",
        "outputId": "5c614986-75d8-4c30-a7dc-cb13a8e98f47"
      },
      "source": [
        "# GPU version\n",
        "x = jax.random.normal(key, (size, size), dtype=jnp.float32)\n",
        "print(type(x))\n",
        "%timeit -o jnp.dot(x, x.T).block_until_ready() "
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'jax.interpreters.xla._DeviceArray'>\n",
            "The slowest run took 7.90 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "1000 loops, best of 3: 719 µs per loop\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TimeitResult : 1000 loops, best of 3: 719 µs per loop>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVLUmoB_9pPV",
        "outputId": "e4780e29-302d-4a74-903c-7ee6c97efd2f"
      },
      "source": [
        "res = _\n",
        "time_gpu = res.best\n",
        "print('GPU time {:0.6f}, CPU time {:0.6f}, speedup {:0.6f}'.format(\n",
        "    time_gpu, time_cpu, time_cpu/time_gpu))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU time 0.000719, CPU time 0.017674, speedup 24.593663\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BO1H-_vSCz8r"
      },
      "source": [
        "We can move numpy arrays to the GPU for speed. The result will be transferred back to CPU for printing, saving, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpxRFku_C9hR",
        "outputId": "11c52f5e-0ce3-40da-f386-a9d37dc11959"
      },
      "source": [
        "from jax import device_put\n",
        "\n",
        "x = np.random.normal(size=(size, size)).astype(np.float32)\n",
        "print(type(x))\n",
        "%timeit np.dot(x, x.T)\n",
        "\n",
        "x = device_put(x)\n",
        "print(type(x))\n",
        "%timeit jnp.dot(x, x.T).block_until_ready()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "100 loops, best of 3: 18.3 ms per loop\n",
            "<class 'jax.interpreters.xla._DeviceArray'>\n",
            "1000 loops, best of 3: 863 µs per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Zsh5DdOF4R1"
      },
      "source": [
        "# Vmap <a class=\"anchor\" id=\"vmap\"></a>\n",
        "\n",
        "\n",
        "To illustrate vmap, consider a binary logistic regression model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XAMcxMsF0-Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b91499e-87f8-4575-d6c6-69b9e5440af4"
      },
      "source": [
        "def sigmoid(x): return 0.5 * (jnp.tanh(x / 2.) + 1)\n",
        "\n",
        "def predict_single(w, x):\n",
        "    return sigmoid(jnp.dot(w, x)) # <(D) , (D)> = (1) # inner product\n",
        "  \n",
        "def predict_batch(w, X):\n",
        "    return sigmoid(jnp.dot(X, w)) # (N,D) * (D,1) = (N,1) # matrix-vector multiply\n",
        "\n",
        "\n",
        "D = 2\n",
        "N = 3\n",
        "\n",
        "#np.random.state(42)\n",
        "#w = np.random.randn(D)\n",
        "#X = np.random.randn(N, D)\n",
        "#y = np.random.randint(0, 2, N)\n",
        "\n",
        "w = jax.random.normal(key, shape=(D,))\n",
        "X = jax.random.normal(key, shape=(N,D))\n",
        "y = jax.random.choice(key, 2, shape=(N,)) # uniform binary labels\n",
        "\n",
        "print(X)\n",
        "print(y)\n",
        "\n",
        "\n",
        "# We can apply predict_batch to a matrix of data, but we cannot apply predict_single in this way\n",
        "# because the order of the arguments to np.dot is incorrect.\n",
        "\n",
        "p1 = predict_batch(w, X)\n",
        "print(p1)\n",
        "try:\n",
        "    p2 = predict_single(w, X)\n",
        "except:\n",
        "    print('cannot apply to batch')"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-1.455  0.973]\n",
            " [-0.217  0.691]\n",
            " [-1.011  0.401]]\n",
            "[0 0 1]\n",
            "[0.938 0.773 0.814]\n",
            "cannot apply to batch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-qWaSNBGIqg"
      },
      "source": [
        "To avoid having to think about batch shape, it is often easier to write a function that works on single\n",
        "input vectors. We can then apply this in a loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFqzy2ZFF7Fc"
      },
      "source": [
        "p3 = [predict_single(w, x) for x in X]\n",
        "assert np.allclose(p1, p3)"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZSksj-1GZcU"
      },
      "source": [
        "Unfortunately, mapping down a list is slow.\n",
        "Fortunately, JAX provides `vmap`, which has the same effect, but can be parallelized.\n",
        "\n",
        "We first apply the `predict_single` function to its first arugment, w, to get a function that only\n",
        "depends on x. We then vectorize this, and map the resulting modified function along rows (dimension 0)\n",
        "of the data matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMNkPb9GGZ1O"
      },
      "source": [
        "from functools import partial\n",
        "\n",
        "predict_single_w = partial(predict_single, w)\n",
        "predict_batch_w = vmap(predict_single_w)\n",
        "p4 = predict_batch_w(X)\n",
        "assert np.allclose(p1, p4)\n",
        "\n",
        "# More concise\n",
        "p5 = vmap(predict_single, in_axes=(None, 0))(w, X)\n",
        "assert np.allclose(p1, p5)\n",
        "\n",
        "p6 = vmap(partial(predict_single, w))(X)\n",
        "assert np.allclose(p1, p6)\n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qA2RYhnNG9hh"
      },
      "source": [
        "# Autograd <a class=\"anchor\" id=\"AD\"></a>\n",
        "\n",
        "In this section, we illustrate automatic differentation using JAX.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTIybB8b4ar0"
      },
      "source": [
        "## Simple convex functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVY7SewS4qlh"
      },
      "source": [
        "from jax import grad, hessian, jacfwd, jacrev, vmap, jit"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xb0gZ_1HBEyC"
      },
      "source": [
        "Linear function: multi-input, scalar output.\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "f(x; a) &= a^T x\\\\\n",
        "\\nabla_x f(x;a) &= a\n",
        "\\end{align}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmYqFEs04vkV"
      },
      "source": [
        "# We construct a single output linear function.\n",
        "# In this case, the Jacobian and gradient are the same.\n",
        "def fun1d(x):\n",
        "    return jnp.dot(a, x)[0]\n",
        "\n",
        "Din = 3; Dout = 1;\n",
        "a = jax.random.normal(key, shape=(Dout, Din))\n",
        "x = jax.random.normal(key, shape=(Din,))\n",
        "g = grad(fun1d)(x)\n",
        "assert np.allclose(g, a)\n",
        "J = jacrev(fun1d)(x)\n",
        "assert np.allclose(J, g)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbgiqkF6BL1E"
      },
      "source": [
        "Linear function: multi-input, multi-output.\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "f(x;A) &= A x \\\\\n",
        "\\nabla_x f(x;A) &= A\n",
        "\\end{align}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6hkEYxV5EIx"
      },
      "source": [
        "# We construct a multi-output linear function.\n",
        "# We check forward and reverse mode give same Jacobians.\n",
        "\n",
        "\n",
        "def fun(x):\n",
        "    return jnp.dot(A, x)\n",
        "\n",
        "Din = 3; Dout = 4;\n",
        "A = jax.random.normal(key, shape=(Dout, Din))\n",
        "x = jax.random.normal(key, shape=(Din,))\n",
        "Jf = jacfwd(fun)(x)\n",
        "Jr = jacrev(fun)(x)\n",
        "assert np.allclose(Jf, Jr)\n",
        "assert np.allclose(Jf, A)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CN5d-D7XBU9Y"
      },
      "source": [
        "Quadratic form.\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "f(x;A) &= x^T A x \\\\\n",
        "\\nabla_x f(x;A) &= (A+A^T) x \\\\\n",
        "\\nabla^2 x^2 f(x;A) &= A + A^T\n",
        "\\end{align}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9URZeX8PBbhl"
      },
      "source": [
        "\n",
        "D = 4\n",
        "A = jax.random.normal(key, shape=(D,D))\n",
        "x = jax.random.normal(key, shape=(D,))\n",
        "\n",
        "quadfun = lambda x: jnp.dot(x, jnp.dot(A, x))\n",
        "\n",
        "J = jacfwd(quadfun)(x)\n",
        "assert np.allclose(J, jnp.dot(A+A.T, x))\n",
        "\n",
        "H1 = hessian(quadfun)(x)\n",
        "assert np.allclose(H1, A+A.T)\n",
        "\n",
        "def my_hessian(fun):\n",
        "  return jacfwd(jacrev(fun))\n",
        "\n",
        "H2 = my_hessian(quadfun)(x)\n",
        "assert np.allclose(H1, H2)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9ZOhDeqCXu3"
      },
      "source": [
        "Chain rule applied to sigmoid function.\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\mu(x;w) &=\\sigma(w^T x) \\\\\n",
        "\\nabla_w \\mu(x;w) &= \\sigma'(w^T x) x \\\\\n",
        "\\sigma'(a) &= \\sigma(a) * (1-\\sigma(a)) \n",
        "\\end{align}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6q5VfLXLB7rv",
        "outputId": "8c822aa3-fae8-4a33-b02e-52c2d29e1e24"
      },
      "source": [
        "\n",
        "\n",
        "D = 4\n",
        "w = jax.random.normal(key, shape=(D,))\n",
        "x = jax.random.normal(key, shape=(D,))\n",
        "y = 0 \n",
        "\n",
        "def sigmoid(x): return 0.5 * (jnp.tanh(x / 2.) + 1)\n",
        "def mu(w): return sigmoid(jnp.dot(w,x))\n",
        "def deriv_mu(w): return mu(w) * (1-mu(w)) * x\n",
        "deriv_mu_jax =  grad(mu)\n",
        "\n",
        "print(deriv_mu(w))\n",
        "print(deriv_mu_jax(w))\n",
        "\n",
        "assert np.allclose(deriv_mu(w), deriv_mu_jax(w), atol=1e-3)\n",
        "\n"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.002 0.003 0.004 0.002]\n",
            "[0.002 0.003 0.004 0.002]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeoGcnV54YY9"
      },
      "source": [
        "## Binary logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Isql2l4MGfIt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de9df57e-c5df-4048-90f5-05db9b6eb0bc"
      },
      "source": [
        "\n",
        "# negative log likelihood\n",
        "def loss(weights, inputs, targets):\n",
        "    preds = predict_batch(weights, inputs)\n",
        "    logprobs = jnp.log(preds) * targets + jnp.log(1 - preds) * (1 - targets)\n",
        "    return -jnp.sum(logprobs)\n",
        "\n",
        "\n",
        "D = 2\n",
        "N = 3\n",
        "w = jax.random.normal(key, shape=(D,))\n",
        "X = jax.random.normal(key, shape=(N,D))\n",
        "y = jax.random.choice(key, 2, shape=(N,)) # uniform binary labels\n",
        "\n",
        "print(loss(w, X, y))\n",
        "\n",
        "# Gradient function\n",
        "grad_fun = grad(loss)\n",
        "\n",
        "# Gradient of each example in the batch - 2 different ways\n",
        "grad_fun_w = partial(grad_fun, w)\n",
        "grads = vmap(grad_fun_w)(X,y)\n",
        "print(grads)\n",
        "assert grads.shape == (N,D)\n",
        "\n",
        "grads2 = vmap(grad_fun, in_axes=(None, 0, 0))(w, X, y) \n",
        "assert np.allclose(grads, grads2)\n",
        "\n",
        "# Gradient for entire batch\n",
        "grad_sum = jnp.sum(grads, axis=0)\n",
        "assert grad_sum.shape == (D,)\n",
        "print(grad_sum)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4.468591\n",
            "[[-1.365  0.913]\n",
            " [-0.168  0.534]\n",
            " [ 0.188 -0.075]]\n",
            "[-1.345  1.373]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3BaHdT4Gj6W",
        "outputId": "2d435ffd-e55d-4c71-c17d-c16ceba986f0"
      },
      "source": [
        "# Textbook implementation of gradient\n",
        "def NLL_grad(weights, batch):\n",
        "    X, y = batch\n",
        "    N = X.shape[0]\n",
        "    mu = predict_batch(weights, X)\n",
        "    g = jnp.sum(jnp.dot(jnp.diag(mu - y), X), axis=0)\n",
        "    return g\n",
        "\n",
        "grad_sum_batch = NLL_grad(w, (X,y))\n",
        "print(grad_sum_batch)\n",
        "assert np.allclose(grad_sum, grad_sum_batch)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-1.345  1.373]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGxDFho3H5ou",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9276a129-b48b-44f5-b242-2221c78788ed"
      },
      "source": [
        "# We can also compute Hessians, as we illustrate below.\n",
        "from jax import hessian\n",
        "\n",
        "hessian_fun = hessian(loss)\n",
        "\n",
        "# Hessian on one example\n",
        "H0 = hessian_fun(w, X[0,:], y[0])\n",
        "print('Hessian(example 0)\\n{}'.format(H0))\n",
        "\n",
        "# Hessian for batch\n",
        "Hbatch = vmap(hessian_fun, in_axes=(None, 0, 0))(w, X, y) \n",
        "print('Hbatch shape {}'.format(Hbatch.shape))\n",
        "\n",
        "Hbatch_sum = jnp.sum(Hbatch, axis=0)\n",
        "print('Hbatch sum\\n {}'.format(Hbatch_sum))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hessian(example 0)\n",
            "[[ 0.123 -0.082]\n",
            " [-0.082  0.055]]\n",
            "Hbatch shape (3, 2, 2)\n",
            "Hbatch sum\n",
            " [[ 0.286 -0.17 ]\n",
            " [-0.17   0.163]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsveS95sIxMh"
      },
      "source": [
        "# Textbook implementation of Hessian\n",
        "\n",
        "def NLL_hessian(weights, batch):\n",
        "  X, y = batch\n",
        "  mu = predict_batch(weights, X)\n",
        "  S = jnp.diag(mu * (1-mu))\n",
        "  H = jnp.dot(jnp.dot(X.T, S), X)\n",
        "  return H\n",
        "\n",
        "H2 = NLL_hessian(w, (X,y) )\n",
        "assert np.allclose(Hbatch_sum, H2, atol=1e-2)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOzQJSX3JOF9"
      },
      "source": [
        "# Vector Jacobian Products\n",
        "\n",
        "Consider a bilinear mapping $f(x,W) = x W$.\n",
        "For fixed parameters, we have\n",
        "$f1(x) = W x$, so $J(x) = W$, and $u^T J(x) = J(x)^T u = W^T u$.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Lpmntn2JREG",
        "outputId": "39778872-aca9-488c-f9dc-40d30a821488"
      },
      "source": [
        "n = 3; m = 2;\n",
        "W = jax.random.normal(key, shape=(m,n))\n",
        "x = jax.random.normal(key, shape=(n,))\n",
        "u = jax.random.normal(key, shape=(m,))\n",
        "\n",
        "def f1(x): return jnp.dot(W,x)\n",
        "\n",
        "J1 = jacfwd(f1)(x)\n",
        "print(J1.shape)\n",
        "\n",
        "assert np.allclose(J1, W)\n",
        "tmp1 = jnp.dot(u.T, J1)\n",
        "print(tmp1)\n",
        "\n",
        "(val, jvp_fun) = jax.vjp(f1, x)\n",
        "tmp2 = jvp_fun(u)\n",
        "assert np.allclose(tmp1, tmp2)\n",
        "\n",
        "tmp3 = np.dot(W.T, u)\n",
        "assert np.allclose(tmp1, tmp3)\n",
        "\n",
        "\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2, 3)\n",
            "[ 0.922  1.216 -0.61 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYSC6DMOO3IS"
      },
      "source": [
        "For fixed inputs, we have\n",
        "$f2(W) = W x$, so $J(W) = \\text{something complex}$,\n",
        "but $u^T J(W) = J(W)^T u = u x^T$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8l3StJdO1r1",
        "outputId": "5a3f1e5b-b903-4db8-bb8a-a53c29bcad19"
      },
      "source": [
        "\n",
        "def f2(W): return jnp.dot(W,x)\n",
        "\n",
        "J2 = jacfwd(f2)(W)\n",
        "print(J2.shape)\n",
        "\n",
        "tmp1 = jnp.dot(u.T, J2)\n",
        "print(tmp1)\n",
        "print(tmp1.shape)\n",
        "\n",
        "(val, jvp_fun) = jax.vjp(f2, W)\n",
        "tmp2 = jvp_fun(u)\n",
        "assert np.allclose(tmp1, tmp2)\n",
        "\n",
        "tmp3 = np.outer(u, x)\n",
        "assert np.allclose(tmp1, tmp3)\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2, 2, 3)\n",
            "[[-1.425  0.379 -0.267]\n",
            " [ 1.555 -0.413  0.291]]\n",
            "(2, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnOIJRGxJigp"
      },
      "source": [
        "\n",
        "# JIT (just in time compilation) <a class=\"anchor\" id=\"JIT\"></a>\n",
        "\n",
        "In this section, we illustrate how to use the Jax JIT compiler to make code go faster (even on a CPU). However, it does not work on arbitrary Python code, as we explain below.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsaHkNovICfd"
      },
      "source": [
        "grad_fun_jit = jit(grad_fun) # speedup gradient function\n",
        "grads_jit = vmap(partial(grad_fun_jit, w))(X,y)\n",
        "assert np.allclose(grads, grads_jit)\n"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGZUbPDLKIkv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72122e59-2a3d-4d3e-9c62-f6bcc3489537"
      },
      "source": [
        "# We can apply JIT to non ML applications as well.\n",
        "\n",
        "def slow_f(x):\n",
        "  # Element-wise ops see a large benefit from fusion\n",
        "  return x * x + x * 2.0\n",
        "\n",
        "x = jnp.ones((5000, 5000))\n",
        "%timeit  slow_f(x) \n",
        "\n",
        "fast_f = jit(slow_f)\n",
        "%timeit fast_f(x)  \n",
        " \n",
        "assert np.allclose(slow_f(x), fast_f(x))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100 loops, best of 3: 5 ms per loop\n",
            "The slowest run took 8.36 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "1000 loops, best of 3: 1.37 ms per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77-33YadKNni"
      },
      "source": [
        "We can also add the `@jit` decorator in front of a function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5L-AEDhAb8t",
        "outputId": "605473e8-4f9d-4437-92da-5d5300698662"
      },
      "source": [
        "@jit\n",
        "def faster_f(x):\n",
        "  return x * x + x * 2.0\n",
        "%timeit faster_f(x)\n",
        "assert np.allclose(faster_f(x), fast_f(x))  "
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The slowest run took 12.84 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "1000 loops, best of 3: 1.37 ms per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdtqCDWZAC2f"
      },
      "source": [
        "## Static argnum\n",
        "\n",
        "Note that JIT compilation requires that the control flow through the function  can be determined by the shape (but not concrete value) of its inputs. The function below violates this, since when x<3, it takes one branch, whereas when x>= 3, it takes the other."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ps1W8LhKKj9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d92f23f4-b0a0-4ccf-cda5-548a1ba354ac"
      },
      "source": [
        "@jit\n",
        "def f(x):\n",
        "  if x < 3:\n",
        "    return 3. * x ** 2\n",
        "  else:\n",
        "    return -4 * x\n",
        "\n",
        "# This will fail!\n",
        "try:\n",
        "  print(f(2))\n",
        "except Exception as e:\n",
        "  print(\"ERROR:\", e)\n",
        "  \n"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR: Abstract tracer value encountered where concrete value is expected.\n",
            "\n",
            "The problem arose with the `bool` function. \n",
            "\n",
            "While tracing the function f at <ipython-input-66-3da05647f18e>:1, this concrete value was not available in Python because it depends on the value of the arguments to f at <ipython-input-66-3da05647f18e>:1 at flattened positions [0], and the computation of these values is being staged out (that is, delayed rather than executed eagerly).\n",
            "\n",
            "You can use transformation parameters such as `static_argnums` for `jit` to avoid tracing particular arguments of transformed functions, though at the cost of more recompiles.\n",
            "\n",
            "See https://jax.readthedocs.io/en/latest/faq.html#abstract-tracer-value-encountered-where-concrete-value-is-expected-error for more information.\n",
            "\n",
            "Encountered tracer value: Traced<ShapedArray(bool[])>with<DynamicJaxprTrace(level=0/1)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tHh4tcXKTRf"
      },
      "source": [
        "We can fix this by telling JAX to trace the control flow through the function using concrete values of some of its arguments. JAX will then compile different versions, depending on the input values. See below for an example.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMaRplccKRHc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e78e39dc-ee95-4dd8-cf58-09836e5b4968"
      },
      "source": [
        "def f(x):\n",
        "  if x < 3:\n",
        "    return 3. * x ** 2\n",
        "  else:\n",
        "    return -4 * x\n",
        "\n",
        "f2 = jit(f, static_argnums=(0,))\n",
        "\n",
        "print(f2(5))"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_Fkav2GRgnc"
      },
      "source": [
        "Unfortunately, the static argnum method fails with vmap, which passes in different inputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHLTdXCSQ6sZ",
        "outputId": "86df9a88-90c6-4ed7-8491-dd00d4dd1833"
      },
      "source": [
        "\n",
        "xs = jnp.arange(5)\n",
        "try:\n",
        "  ys = vmap(f)(xs)\n",
        "  print('used vmap')\n",
        "except:\n",
        "  ys = jnp.array([f(x) for x in xs])\n",
        "  print('did not use vmap')\n",
        "print(ys)\n",
        "\n"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "did not use vmap\n",
            "[  0.   3.  12. -12. -16.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-L-dB_GBWNq"
      },
      "source": [
        "## Side effects"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2zrYp3-KcMc"
      },
      "source": [
        "There are a few other subtleties. If your function has global side-effects, JAX's tracer can cause weird things to happen. A common gotcha is trying to print arrays inside jit'd functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LC5q9OCSTAKQ",
        "outputId": "23423266-c77b-4d56-bb4a-4b0fe767e768"
      },
      "source": [
        "def f(x):\n",
        "  print(x)\n",
        "  y = 2 * x\n",
        "  print(y)\n",
        "  return y\n",
        "y1 = f(2)\n",
        "print(y1)\n",
        "\n",
        "print('jit version follows')\n",
        "@jit\n",
        "def f(x):\n",
        "  print(x)\n",
        "  y = 2 * x\n",
        "  print(y)\n",
        "  return y\n",
        "y2 = f(2)\n",
        "print(y2)\n",
        "\n",
        "print('call jitted function a second time')\n",
        "y2 = f(2)\n",
        "print(y2)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "4\n",
            "4\n",
            "jit version follows\n",
            "Traced<ShapedArray(int32[], weak_type=True)>with<DynamicJaxprTrace(level=0/1)>\n",
            "Traced<ShapedArray(int32[])>with<DynamicJaxprTrace(level=0/1)>\n",
            "4\n",
            "call jitted function a second time\n",
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtZrmURWVPxF"
      },
      "source": [
        "# Worked example: gradient descent for linear regression\n",
        "\n",
        "We put some of the above pieces together to show how to implement (batch) gradient descent to minimize squared error on a linear model. The code is based on the [flax JAX tutorial](https://flax.readthedocs.io/en/latest/notebooks/jax_for_the_impatient.html). We choose a simple example because we will modify this code later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUhQwlsaVjdT"
      },
      "source": [
        "\n",
        "\n",
        "# Create the predict function from a set of parameters\n",
        "def make_predict_fun(W,b):\n",
        "  def predict(x):\n",
        "    return jnp.dot(W,x)+b\n",
        "  return predict\n",
        "\n",
        "# Create the loss from the data points set\n",
        "def make_mse_fun(x_batched,y_batched): # returns fn(W,b)\n",
        "  def mse(W,b):\n",
        "    # Define the squared loss for a single pair (x,y)\n",
        "    def squared_error(x,y):\n",
        "      y_pred = make_predict_fun(W,b)(x)\n",
        "      return jnp.inner(y-y_pred,y-y_pred)/2.0\n",
        "    # We vectorize the previous to compute the average of the loss on all samples.\n",
        "    return jnp.mean(jax.vmap(squared_error)(x_batched,y_batched), axis=0)\n",
        "  return jax.jit(mse) # And finally we jit the result."
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vj1Ay_GDVn-r"
      },
      "source": [
        "# Set problem dimensions\n",
        "N = 20\n",
        "xdim = 10\n",
        "ydim = 5\n",
        "\n",
        "# Generate random ground truth W and b\n",
        "key = random.PRNGKey(0)\n",
        "Wtrue = random.normal(key, (ydim, xdim))\n",
        "btrue = random.normal(key, (ydim,))\n",
        "true_predict_fun = make_predict_fun(Wtrue, btrue)\n",
        "\n",
        "# Generate data with additional observation noise\n",
        "X = random.normal(key, (N, xdim))\n",
        "Ytrue = jax.vmap(true_predict_fun)(X)\n",
        "Y = Ytrue + 0.1*random.normal(key, (N, ydim))\n",
        "\n",
        "# Generate MSE for our samples\n",
        "mse_fun = make_mse_fun(X, Y)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrFRYzqwWgIm",
        "outputId": "fe51d014-a740-4fc3-fcbc-0911eab01700"
      },
      "source": [
        "# Initialize estimated W and b with zeros.\n",
        "What = jnp.zeros_like(Wtrue)\n",
        "bhat = jnp.zeros_like(btrue)\n",
        "\n",
        "alpha = 0.3 # Gradient step size\n",
        "for i in range(101):\n",
        "  grad_W = jax.grad(mse_fun,0)(What,bhat)\n",
        "  grad_b = jax.grad(mse_fun,1)(What,bhat)\n",
        "  What = What - alpha*grad_W\n",
        "  bhat = bhat - alpha*grad_b \n",
        "  if (i%10==0):\n",
        "    print(\"Loss step {}: \".format(i), mse_fun(What,bhat))\n",
        "\n",
        "assert np.allclose(Wtrue, What, atol=1e-1)\n",
        "assert np.allclose(btrue, bhat, atol=1e-1)\n",
        "\n",
        "\n",
        "print('loss with true params {}, loss with estimated params {}'.format(\n",
        "    mse_fun(Wtrue, btrue), mse_fun(What, bhat)))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss step 0:  6.5597453\n",
            "Loss step 10:  0.17232795\n",
            "Loss step 20:  0.043397333\n",
            "Loss step 30:  0.024473595\n",
            "Loss step 40:  0.01707891\n",
            "Loss step 50:  0.013489492\n",
            "Loss step 60:  0.011695366\n",
            "Loss step 70:  0.0107952645\n",
            "Loss step 80:  0.0103434585\n",
            "Loss step 90:  0.010116665\n",
            "Loss step 100:  0.010002807\n",
            "loss with true params 0.02229204587638378, loss with estimated params 0.010002806782722473\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gctKo1NjJzQ1"
      },
      "source": [
        "# Pytrees\n",
        "\n",
        "A Pytree is a a nested datastructure, such as a list or tuple, which contains items (eg arrays or strings) at its leaves. It is useful for representing hierarchical sets of parameters for DNNs (and other structured dsta). \n",
        "\n",
        "We can map functions down a pytree in the same way that we can map a function down a list. We can also combine elements in two pytrees that have the same shape to make a third pytree. We illustrate this below, following the [flax Jax tutorial](https://flax.readthedocs.io/en/latest/notebooks/jax_for_the_impatient.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGxWcHWJKtsq"
      },
      "source": [
        "## Simple example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1EllUvlJ1NW",
        "outputId": "8afe8c28-c938-4e4d-f1d8-146ef9231728"
      },
      "source": [
        "from jax import tree_util\n",
        "\n",
        "# a simple pytree\n",
        "t1 = [1, {\"k1\": 2, \"k2\": (3, 4)}, 5]\n",
        "\n",
        "t2 = tree_util.tree_map(lambda x: x*x, t1)\n",
        "print(t2)\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, {'k1': 4, 'k2': (9, 16)}, 25]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0y3UFtJZKr08",
        "outputId": "8fbd44cc-7e8e-4d8c-ab43-fd36a27e14b5"
      },
      "source": [
        "t3 = tree_util.tree_multimap(lambda x,y: x+y, t1, t2)\n",
        "print(t3)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2, {'k1': 6, 'k2': (12, 20)}, 30]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8iMrvUXK8Id"
      },
      "source": [
        "## More complex example: linear regression revisited"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77GAXfBmLByE"
      },
      "source": [
        "\n",
        "# Create the predict function from a set of parameters\n",
        "def make_predict_pytree(params):\n",
        "  def predict(x):\n",
        "    return jnp.dot(params['W'],x)+params['b']\n",
        "  return predict\n",
        "\n",
        "# Create the loss from the data points set\n",
        "def make_mse_pytree(x_batched,y_batched): # returns fn(params)->real\n",
        "  def mse(params):\n",
        "    # Define the squared loss for a single pair (x,y)\n",
        "    def squared_error(x,y):\n",
        "      y_pred = make_predict_pytree(params)(x)\n",
        "      return jnp.inner(y-y_pred,y-y_pred)/2.0\n",
        "    # We vectorize the previous to compute the average of the loss on all samples.\n",
        "    return jnp.mean(jax.vmap(squared_error)(x_batched,y_batched), axis=0)\n",
        "  return jax.jit(mse) # And finally we jit the result."
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0YusY89LGS8",
        "outputId": "5f9f9603-cf16-422e-8345-d155192a70e0"
      },
      "source": [
        "# Initialize estimated W and b with zeros.\n",
        "params = {'W': jnp.zeros_like(Wtrue), 'b': jnp.zeros_like(btrue)}\n",
        "params_true = {'W': Wtrue, 'b': btrue}\n",
        "\n",
        "mse_pytree = make_mse_pytree(X, Y)\n",
        "print(mse_pytree(params_true))\n",
        "print(mse_pytree(params))\n",
        "\n",
        "print(jax.grad(mse_pytree)(params))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.022292046\n",
            "24.97824\n",
            "{'W': DeviceArray([[-0.039,  0.755,  0.542,  0.36 ,  0.224,  1.651,  1.534,\n",
            "              -1.342, -0.15 , -1.638],\n",
            "             [-0.324,  0.141, -0.402,  0.498,  1.829,  4.308,  2.138,\n",
            "              -2.43 , -0.381, -2.178],\n",
            "             [ 1.7  , -0.707, -0.656, -0.568,  1.824, -2.194, -0.477,\n",
            "               0.96 ,  1.622,  1.408],\n",
            "             [-0.862,  0.321, -0.388, -0.74 , -0.82 ,  0.441,  0.772,\n",
            "              -1.713, -1.592, -0.557],\n",
            "             [ 1.338, -0.632, -0.968, -1.127,  1.775,  0.323,  1.405,\n",
            "              -0.638,  1.077, -0.739]], dtype=float32), 'b': DeviceArray([ 0.036,  1.092, -0.413, -1.389, -0.862], dtype=float32)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WA0ZjMofMFW6",
        "outputId": "b47c6985-2eeb-476e-cc45-ef0e6f3ed438"
      },
      "source": [
        "alpha = 0.3 # Gradient step size\n",
        "print('Loss for \"true\" W,b: ', mse_pytree(params_true))\n",
        "for i in range(101):\n",
        "  gradients = jax.grad(mse_pytree)(params)\n",
        "  params = jax.tree_multimap(lambda old,grad: old-alpha*grad, params, gradients)\n",
        "  if (i%10==0):\n",
        "    print(\"Loss step {}: \".format(i), mse_pytree(params))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss for \"true\" W,b:  0.022292046\n",
            "Loss step 0:  6.5597453\n",
            "Loss step 10:  0.17232795\n",
            "Loss step 20:  0.043397333\n",
            "Loss step 30:  0.024473595\n",
            "Loss step 40:  0.01707891\n",
            "Loss step 50:  0.013489492\n",
            "Loss step 60:  0.011695366\n",
            "Loss step 70:  0.0107952645\n",
            "Loss step 80:  0.0103434585\n",
            "Loss step 90:  0.010116665\n",
            "Loss step 100:  0.010002807\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDXJO4FdUPJM"
      },
      "source": [
        "# Looping constructs\n",
        "\n",
        "For loops in Python are slow, even when JIT-compiled. However, there are built-in primitives for loops that are fast, as we illustrate below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-wtZiZmU329"
      },
      "source": [
        "## For loops.\n",
        "\n",
        "The semantics of the for loop function in JAX is as follows:\n",
        "```\n",
        "def fori_loop(lower, upper, body_fun, init_val):\n",
        "  val = init_val\n",
        "  for i in range(lower, upper):\n",
        "    val = body_fun(i, val)\n",
        "  return val\n",
        "```\n",
        "We see that ```val``` is used to accumulate the results across iterations.\n",
        "\n",
        "Below is an example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIlLD0kKVGya"
      },
      "source": [
        "# sum from 1 to N = N*(N+1)/2\n",
        "\n",
        "def sum_exact(N):\n",
        "  return int(N*(N+1)/2)\n",
        "\n",
        "def sum_slow(N):\n",
        "  s = 0\n",
        "  for i in range(1,N+1):\n",
        "    s += i\n",
        "  return s\n",
        "\n",
        "N = 10\n",
        "\n",
        "assert sum_slow(N) == sum_exact(N)\n",
        "\n",
        "def sum_fast(N):\n",
        "  s = jax.lax.fori_loop(1, N+1, lambda i,partial_sum: i+partial_sum, 0)\n",
        "  return s\n",
        "\n",
        "assert sum_fast(N) == sum_exact(N) "
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWuOBk9SRLJb",
        "outputId": "b3cf35ae-b82b-461b-8ea4-86cc7fb93575"
      },
      "source": [
        "N = 1000\n",
        "%timeit sum_slow(N)\n",
        "%timeit sum_fast(N)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000 loops, best of 3: 44.1 µs per loop\n",
            "10 loops, best of 3: 41 ms per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSH6VQLHRWfA",
        "outputId": "01bd0110-0e55-4038-d04f-9074741e71c5"
      },
      "source": [
        "N = 100000\n",
        "%timeit sum_slow(N)\n",
        "%timeit sum_fast(N)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100 loops, best of 3: 5.04 ms per loop\n",
            "1 loop, best of 3: 2.88 s per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvUsG4mUR3Sl"
      },
      "source": [
        "# Let's do more compute per step of the for loop\n",
        "\n",
        "D = 10\n",
        "X = jax.random.normal(key, shape=(D,D))\n",
        "\n",
        "def sum_slow(N):\n",
        "  s = jnp.zeros_like(X)\n",
        "  for i in range(1,N+1):\n",
        "    s += jnp.dot(X, X)\n",
        "  return s\n",
        "\n",
        "def sum_fast(N):\n",
        "  s = jnp.zeros_like(X)\n",
        "  s = jax.lax.fori_loop(1, N+1, lambda i,s: s+jnp.dot(X,X), s)\n",
        "  return s\n",
        "\n",
        "N = 10\n",
        "assert np.allclose(sum_fast(N), sum_slow(N))"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8NtevKvS8F_",
        "outputId": "a330fe55-f3c9-4854-90f5-f177541f5b66"
      },
      "source": [
        "N = 1000\n",
        "%timeit sum_slow(N)\n",
        "%timeit sum_fast(N)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 loop, best of 3: 482 ms per loop\n",
            "10 loops, best of 3: 46.3 ms per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ru5D0tZEiV2e"
      },
      "source": [
        "## While loops\n",
        "\n",
        "Here is the semantics of the JAX while loop\n",
        "\n",
        "\n",
        "```\n",
        "def while_loop(cond_fun, body_fun, init_val):\n",
        "  val = init_val\n",
        "  while cond_fun(val):\n",
        "    val = body_fun(val)\n",
        "  return val\n",
        "```\n",
        "\n",
        "Below is an example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWKhfJTDiiSI"
      },
      "source": [
        "\n",
        "\n",
        "def sum_slow_while(N):\n",
        "  s = 0\n",
        "  i = 0\n",
        "  while (i <= N):\n",
        "    s += i\n",
        "    i += 1\n",
        "  return s\n",
        "\n",
        "\n",
        "def sum_fast_while(N):\n",
        "  init_val = (0,0)\n",
        "  def cond_fun(val):\n",
        "    s,i = val\n",
        "    return i<=N\n",
        "  def body_fun(val):\n",
        "    s,i = val\n",
        "    s += i\n",
        "    i += 1\n",
        "    return (s,i)\n",
        "  val = jax.lax.while_loop(cond_fun, body_fun, init_val)\n",
        "  s2 = val[0]\n",
        "  return s2\n",
        "\n",
        "N = 10\n",
        "assert sum_slow_while(N) == sum_exact(N)\n",
        "assert sum_slow_while(N) == sum_fast_while(N)"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AieugSOZLGi5"
      },
      "source": [
        "# Mutation of arrays \n",
        "\n",
        "Since JAX is functional, you cannot mutate arrays in place,\n",
        "since this makes program analysis and transformation very difficult. JAX requires a pure functional expression of a numerical program.\n",
        "Instead, JAX offers the functional update functions: `index_update`, `index_add`, `index_min`, `index_max`, and the `index` helper. These are illustrated below. \n",
        "\n",
        "Note: If the input values of `index_update` aren't reused, jit-compiled code will perform these operations in-place, rather than making a copy. \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEKfhvrTLEBf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c263bc2-4431-48ee-b805-c89e4bb47b87"
      },
      "source": [
        "# You cannot assign directly to elements of an array.\n",
        "\n",
        "A = jnp.zeros((3,3), dtype=np.float32)\n",
        "\n",
        "# In place update of JAX's array will yield an error!\n",
        "try:\n",
        "  A[1, :] = 1.0\n",
        "except:\n",
        "  print('must use index_update')"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "must use index_update\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9P3uUMzXVjw9",
        "outputId": "0dfc73a6-5d63-4364-a0eb-e966bb821b01"
      },
      "source": [
        "from jax.ops import index, index_add, index_update\n",
        "\n",
        "D = 3\n",
        "A = 2*jnp.ones((D,D))\n",
        "print(\"original array:\")\n",
        "print(A)\n",
        "\n",
        "A2 = index_update(A, index[1, :], 42.0) # A[1,:] = 42\n",
        "print(\"original array:\")\n",
        "print(A) # unchanged\n",
        "print(\"new array:\")\n",
        "print(A2)\n",
        "\n",
        "A3 = A.at[1,:].set(42.0) # A3=np.copy(A),  A3[1,:] = 42\n",
        "print(\"original array:\")\n",
        "print(A) # unchanged\n",
        "print(\"new array:\")\n",
        "print(A3)\n",
        "\n",
        "A4 = A.at[1,:].mul(42.0) # A4=np.copy(A),  A4[1,:] *= 42\n",
        "print(\"original array:\")\n",
        "print(A) # unchanged\n",
        "print(\"new array:\")\n",
        "print(A4)\n",
        "\n"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original array:\n",
            "[[2. 2. 2.]\n",
            " [2. 2. 2.]\n",
            " [2. 2. 2.]]\n",
            "original array:\n",
            "[[2. 2. 2.]\n",
            " [2. 2. 2.]\n",
            " [2. 2. 2.]]\n",
            "new array:\n",
            "[[ 2.  2.  2.]\n",
            " [42. 42. 42.]\n",
            " [ 2.  2.  2.]]\n",
            "original array:\n",
            "[[2. 2. 2.]\n",
            " [2. 2. 2.]\n",
            " [2. 2. 2.]]\n",
            "new array:\n",
            "[[ 2.  2.  2.]\n",
            " [42. 42. 42.]\n",
            " [ 2.  2.  2.]]\n",
            "original array:\n",
            "[[2. 2. 2.]\n",
            " [2. 2. 2.]\n",
            " [2. 2. 2.]]\n",
            "new array:\n",
            "[[ 2.  2.  2.]\n",
            " [84. 84. 84.]\n",
            " [ 2.  2.  2.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwmI9DH2K_nl"
      },
      "source": [
        "# Implicitly casting lists to vectors\n",
        "\n",
        "You cannot treat a list of numbers as a vector. Instead you must explicitly create the vector using the np.array() constructor.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUURw01jKnXC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1884649d-788a-4533-9bc3-2813b23e55f7"
      },
      "source": [
        "# You cannot treat a list of numbers as a vector. \n",
        "try:\n",
        "  S = jnp.diag([1.0, 2.0, 3.0])\n",
        "except:\n",
        "  print('must convert indices to np.array')"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "must convert indices to np.array\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mMgaegpLCYw"
      },
      "source": [
        "# Instead you should explicitly construct the vector.\n",
        "\n",
        "S = jnp.diag(jnp.array([1.0, 2.0, 3.0]))"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6G5zdgRxGUE3"
      },
      "source": [
        "# JAX neural net libraries\n",
        "\n",
        "JAX is a purely functional library, which differs from Tensorflow and\n",
        "Pytorch, which are stateful. The main advantages of functional programming\n",
        "are that  we can safely transform the code, and/or run it in parallel, without worrying about\n",
        "global state changing behind the scenes. The main disadvantage is that code (especially DNNs) can be harder to write.\n",
        "To simplify the task, various DNN libraries have been designed, as we list below. In this book, we use Flax.\n",
        "\n",
        "|Name|Description|\n",
        "|----|----|\n",
        "|[Stax](https://github.com/google/jax/blob/master/jax/experimental/stax.py)|Barebones library for specifying DNNs|\n",
        "|[Flax](https://github.com/google/flax)|Library for specifying and training DNNs|\n",
        "|[Haiku](https://github.com/deepmind/dm-haiku)|Library for specifying DNNs, similar to Sonnet|\n",
        "|[Trax](https://github.com/google/trax)|Library for specifying and training DNNs, with a focus on sequence models|\n",
        "|[Objax](https://github.com/google/objax)|Stateful (object-oriented) DNN framework, similar to PyTorch, not compatible with other JAX libraries|\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-JiigmlGeBB"
      },
      "source": [
        "# Other JAX  libraries\n",
        "\n",
        "There are many other useful JAX libraries, some of which we list below.\n",
        "\n",
        "|Name|Description|\n",
        "|----|----|\n",
        "|[NumPyro](https://github.com/pyro-ppl/numpyro)|Library for (deep) probabilistic modeling|\n",
        "|[Optax](https://github.com/deepmind/optax)|Library for defining gradient-based optimizers|\n",
        "|[RLax](https://github.com/deepmind/rlax)|Library for reinforcement learning|\n",
        "|[Chex](https://github.com/deepmind/chex)|Library for debugging and developing reliable JAX code|"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7U9Ee15LLC-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "flax_intro.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMQpILOuTVDHnie6tYa4BLX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d7f80b4703ea49f68156b9a2d2340692": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e0a4d052d22a4d47b516b95c8ac3acc5",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a157229195ab41d18d8d937d8750f800",
              "IPY_MODEL_b8db32e87d7140ae93665d859f71e3c5"
            ]
          }
        },
        "e0a4d052d22a4d47b516b95c8ac3acc5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a157229195ab41d18d8d937d8750f800": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f41bf22b33b3424c9acaf2f2fe5102a0",
            "_dom_classes": [],
            "description": "Dl Completed...: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 4,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 4,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e6d5accc6bfd406a9364bf2aff28af70"
          }
        },
        "b8db32e87d7140ae93665d859f71e3c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_fde4e6bb53824791801dbbe6e804019e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4/4 [00:01&lt;00:00,  2.03 file/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f1525b505268487a996471820838d64b"
          }
        },
        "f41bf22b33b3424c9acaf2f2fe5102a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e6d5accc6bfd406a9364bf2aff28af70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fde4e6bb53824791801dbbe6e804019e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f1525b505268487a996471820838d64b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/probml/pyprobml/blob/master/book1/mlp/flax_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rF208fIxvq8m"
      },
      "source": [
        "# Introduction to neural networks using Flax\n",
        "\n",
        "\n",
        "\n",
        "Flax / Linen is a neural net library, built on top of JAX, \"designed to offer an implicit variable management API to save the user from having to manually thread thousands of variables through a complex tree of functions.\" To handle both current and future JAX transforms (configured and composed in any way), Linen Modules are defined as explicit functions of the form\n",
        "$$\n",
        "f(v_{in}, x) \\rightarrow v_{out}, y\n",
        "$$\n",
        "Where $v_{in}$ is the collection of variables (eg. parameters) and PRNG state used by the model, $v_{out}$ the mutated output variable collections, $x$ the input data and $y$ the output data. We illustrate this below. Our tutorial is based on the official [flax intro](https://flax.readthedocs.io/en/latest/notebooks/flax_basics.html) and [linen colab](https://github.com/google/flax/blob/master/docs/notebooks/linen_intro.ipynb). Details are in the [flax source code](https://flax.readthedocs.io/en/latest/_modules/index.html). Note: please be sure to read our [JAX tutorial](https://github.com/probml/pyprobml/blob/master/book1/intro/jax_intro.ipynb) first.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRAzAXYXvztz"
      },
      "source": [
        "import numpy as np\n",
        "#np.set_printoptions(precision=3)\n",
        "np.set_printoptions(formatter={'float': lambda x: \"{0:0.5f}\".format(x)})\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pandas as pd\n",
        "pd.set_option('precision', 2) # 2 decimal places\n",
        "pd.set_option('display.max_rows', 20)\n",
        "pd.set_option('display.max_columns', 30)\n",
        "pd.set_option('display.width', 100) # wide windows"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ob8P9ALvkcM",
        "outputId": "573415d3-eba0-4d6a-e51a-674aea067217"
      },
      "source": [
        "# Install the latest JAXlib version.\n",
        "!pip install --upgrade -q pip jax jaxlib"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34.0 MB 115 kB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68kI74E1vvEI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c59c7062-040c-4307-ad17-f582a336569d"
      },
      "source": [
        "import jax\n",
        "from typing import Any, Callable, Dict, Iterator, Mapping, Optional, Sequence, Tuple\n",
        "from jax import lax, random, numpy as jnp\n",
        "key = random.PRNGKey(0)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3dXu6XY6U0H"
      },
      "source": [
        "# Useful type aliases\n",
        "\n",
        "Array = jnp.ndarray\n",
        "PRNGKey = Array\n",
        "Batch = Mapping[str, np.ndarray]\n",
        "OptState = Any"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pcNcE9_Qj_l",
        "outputId": "df0077d8-93cf-4153-c0c6-9d98e780b2be"
      },
      "source": [
        "# Install Flax at head:\n",
        "!pip install --upgrade -q git+https://github.com/google/flax.git"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for flax (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s80k9sonQfDi"
      },
      "source": [
        "import flax\n",
        "from flax.core import freeze, unfreeze\n",
        "from flax import linen as nn\n",
        "from flax import optim\n",
        "\n",
        "from jax.config import config\n",
        "config.enable_omnistaging() # Linen requires enabling omnistaging"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGrUFJYxjyL7"
      },
      "source": [
        "# MLP in vanilla JAX\n",
        "\n",
        "We construct a simple MLP with L hidden layers (relu activation), and scalar output (linear activation).\n",
        "\n",
        "Note: JAX and Flax, like NumPy, are row-based systems, meaning that vectors are represented as row vectors and not column vectors. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWQGVJMP0VMB"
      },
      "source": [
        "# We define the parameter initializers using a signature that is flax-compatible\n",
        "# https://flax.readthedocs.io/en/latest/_modules/jax/_src/nn/initializers.html\n",
        "\n",
        "def weights_init(key, shape, dtype=jnp.float32):\n",
        "  return random.normal(key, shape, dtype)\n",
        "  #return jnp.ones(shape, dtype)\n",
        "\n",
        "def bias_init(key, shape, dtype=jnp.float32):\n",
        "  return jnp.zeros(shape, dtype)\n",
        "\n",
        "def relu(a):\n",
        "  return jnp.maximum(a, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GepkhhTh-9b-"
      },
      "source": [
        "# A minimal MLP class\n",
        "\n",
        "class MLP0():\n",
        "  features: Sequence[int] # number of features in each layer\n",
        "\n",
        "  def __init__(self, features): # class constructor\n",
        "    self.features = features\n",
        "\n",
        "  def init(self, key, x): # initialize parameters\n",
        "    in_size = np.shape(x)[1]\n",
        "    sizes = np.concatenate( ([in_size], self.features) )\n",
        "    nlayers = len(sizes)\n",
        "    params = {}\n",
        "    for i in range(nlayers-1):\n",
        "      in_size = sizes[i]\n",
        "      out_size = sizes[i+1]\n",
        "      subkey1, subkey2, key = random.split(key, num=3)\n",
        "      W = weights_init(subkey1, (in_size, out_size) )\n",
        "      b = bias_init(subkey2, out_size)\n",
        "      params[f'W{i}'] = W\n",
        "      params[f'b{i}'] = b\n",
        "    return params\n",
        "\n",
        "  def apply(self, params, x): # forwards pass\n",
        "    activations = x\n",
        "    nhidden_layers = len(self.features)-1\n",
        "    for i in range(nhidden_layers):\n",
        "      W = params[f'W{i}']; b = params[f'b{i}'];\n",
        "      outputs = jnp.dot(activations, W) + b\n",
        "      activations = relu(outputs)\n",
        "    # for final layer, no activation function\n",
        "    i = nhidden_layers\n",
        "    outputs = jnp.dot(activations, params[f'W{i}']) + params[f'b{i}']\n",
        "    return outputs\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jFS4SNO0V_I",
        "outputId": "c9ea7bcd-9f31-48f5-be7e-aec6d11a265f"
      },
      "source": [
        "key = random.PRNGKey(0)\n",
        "D = 3\n",
        "N = 2\n",
        "x = random.normal(key, (N,D,))\n",
        "layer_sizes = [3,1] # 1 hidden layer of size 3, 1 scalar output\n",
        "\n",
        "\n",
        "model0 = MLP0(layer_sizes)\n",
        "params0 = model0.init(key, x)\n",
        "\n",
        "print('params')\n",
        "for k,v in params0.items():\n",
        "  print(k, v.shape)\n",
        "  print(v)\n",
        "\n",
        "y0 = model0.apply(params0, x)\n",
        "print('\\noutput')\n",
        "print(y0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "params\n",
            "W0 (3, 3)\n",
            "[[-1.83021 1.18417 0.06777]\n",
            " [0.34588 0.37858 -0.65318]\n",
            " [0.18976 0.45157 -0.33964]]\n",
            "b0 (3,)\n",
            "[0.00000 0.00000 0.00000]\n",
            "W1 (3, 1)\n",
            "[[-1.74905]\n",
            " [1.83313]\n",
            " [-0.23808]]\n",
            "b1 (1,)\n",
            "[0.00000]\n",
            "\n",
            "output\n",
            "[[-0.09538]\n",
            " [2.78382]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBtPT-drBkGA"
      },
      "source": [
        "# Our first flax model\n",
        "\n",
        "Here we recreate the vanilla model in flax. Since we don't specify how the parameters are initialized, the behavior will not be identical to the vanilla model --- we will fix this below, but for now, we focus on model construction.\n",
        "\n",
        "We see that the model is a subclass of `nn.Module`, which is a subclass of Python's dataclass. The child class (written by the user) must define a `model.call(inputs)` method, that applies the function to the input, and a `model.setup()` method, that creates the modules inside this model.\n",
        "\n",
        "The module (parent) class defines two main methods: `model.apply(variables, input`, that applies the function to the input (and variables) to generate an output; and `model.init(key, input)`, that initializes the variables and returns them as a \"frozen dictionary\". This dictionary can contain multiple *kinds* of variables. In the example below, the only kind are parameters, which are immutable variables (that will usually get updated in an external optimization loop, as we show later). The parameters are  automatically named after the corresponding module (here, dense0, dense1, etc).  In this example, both modules are dense layers, so their parameters are a weight matrix (called 'kernel') and a bias vector.\n",
        "\n",
        "The hyper-parameters (in this case, the size of each layer) are stored as attributes of the class, and are specified when the module is constructed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zueDo1r0Qav"
      },
      "source": [
        "class MLP(nn.Module):\n",
        "  features: Sequence[int]\n",
        "  default_attr: int = 42\n",
        "\n",
        "  def setup(self):\n",
        "    print('setup')\n",
        "    self.layers = [nn.Dense(feat) for feat in self.features]\n",
        "\n",
        "  def __call__(self, inputs):\n",
        "    print('call')\n",
        "    x = inputs\n",
        "    for i, lyr in enumerate(self.layers):\n",
        "      x = lyr(x)\n",
        "      if i != len(self.layers) - 1:\n",
        "        x = nn.relu(x)\n",
        "    return x\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoYDn8lX7_ZH",
        "outputId": "c3b9c07a-b5df-4788-af9a-6fb72b52a537"
      },
      "source": [
        "key = random.PRNGKey(0)\n",
        "D = 3\n",
        "N = 2\n",
        "x = random.normal(key, (N,D,))\n",
        "layer_sizes = [3,1] # 1 hidden layer of size 3, 1 scalar output\n",
        "\n",
        "print('calling constructor')\n",
        "model = MLP(layer_sizes) # just initialize attributes of the object\n",
        "print('OUTPUT')\n",
        "print(model)\n",
        "\n",
        "print('\\ncalling init')\n",
        "variables = model.init(key, x)  # calls setup then __call___\n",
        "print('OUTPUT')\n",
        "print(variables)\n",
        "\n",
        "print('\\nW0')\n",
        "W0 = variables['params']['layers_0']['kernel']\n",
        "print(W0)\n",
        "\n",
        "print('Calling apply')\n",
        "y = model.apply(variables, x) # calls setup then __call___\n",
        "print(y)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "calling constructor\n",
            "OUTPUT\n",
            "MLP(\n",
            "    # attributes\n",
            "    features = [3, 1]\n",
            "    default_attr = 42\n",
            ")\n",
            "\n",
            "calling init\n",
            "setup\n",
            "call\n",
            "OUTPUT\n",
            "FrozenDict({\n",
            "    params: {\n",
            "        layers_0: {\n",
            "            kernel: DeviceArray([[0.57725, 0.43926, 0.69045],\n",
            "                         [0.02542, 0.50461, 0.56675],\n",
            "                         [0.07185, 0.17350, -0.04227]], dtype=float32),\n",
            "            bias: DeviceArray([0.00000, 0.00000, 0.00000], dtype=float32),\n",
            "        },\n",
            "        layers_1: {\n",
            "            kernel: DeviceArray([[0.24313],\n",
            "                         [0.94535],\n",
            "                         [-0.12602]], dtype=float32),\n",
            "            bias: DeviceArray([0.00000], dtype=float32),\n",
            "        },\n",
            "    },\n",
            "})\n",
            "\n",
            "W0\n",
            "[[0.57725 0.43926 0.69045]\n",
            " [0.02542 0.50461 0.56675]\n",
            " [0.07185 0.17350 -0.04227]]\n",
            "Calling apply\n",
            "setup\n",
            "call\n",
            "[[0.02978]\n",
            " [0.66403]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lwM1j1WksDG"
      },
      "source": [
        "# Compact modules\n",
        "\n",
        "To reduce the amount of boiler plate code, flax makes it possible to define a module just by writing the `call` method, avoiding the need to write a `setup` function. The corresponding layers will be created when the `init` funciton is called, so the input shape can be inferred lazily (when passed an input). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Akq_iXXdktwb",
        "outputId": "1e9e80e3-9bdf-4653-bd7d-7266bd1424b5"
      },
      "source": [
        "class MLP(nn.Module):\n",
        "  features: Sequence[int]\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, inputs):\n",
        "    x = inputs\n",
        "    for i, feat in enumerate(self.features):\n",
        "      x = nn.Dense(feat)(x)\n",
        "      if i != len(self.features) - 1:\n",
        "        x = nn.relu(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "model = MLP(layer_sizes)\n",
        "print(model)\n",
        "\n",
        "params = model.init(key, x)\n",
        "print(params)\n",
        "\n",
        "y = model.apply(params, x)\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MLP(\n",
            "    # attributes\n",
            "    features = [3, 1]\n",
            ")\n",
            "FrozenDict({\n",
            "    params: {\n",
            "        Dense_0: {\n",
            "            kernel: DeviceArray([[0.28216, 1.03322, 0.07901],\n",
            "                         [0.15159, -0.50100, -0.22373],\n",
            "                         [-0.40327, -0.39875, -0.09402]], dtype=float32),\n",
            "            bias: DeviceArray([0.00000, 0.00000, 0.00000], dtype=float32),\n",
            "        },\n",
            "        Dense_1: {\n",
            "            kernel: DeviceArray([[0.25432],\n",
            "                         [0.76792],\n",
            "                         [0.48329]], dtype=float32),\n",
            "            bias: DeviceArray([0.00000], dtype=float32),\n",
            "        },\n",
            "    },\n",
            "})\n",
            "[[0.56035]\n",
            " [1.07065]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNiuZ54yB7Gj"
      },
      "source": [
        "# Explicit parameter initialization\n",
        "\n",
        "We can control the initialization of the random parameters in each submodule by specifying an init function. Below we show how to initialize our MLP to match the vanilla JAX model. We then check both methods give the same outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5W_lEFsU4t04",
        "outputId": "f44c742a-c919-4fd3-9fa9-b659c246f102"
      },
      "source": [
        "def make_const_init(x):\n",
        "  def init_params(key, shape, dtype=jnp.float32):\n",
        "    return x\n",
        "  return init_params\n",
        "\n",
        "class MLP_init(nn.Module):\n",
        "  features: Sequence[int]\n",
        "  params_init: Dict\n",
        "\n",
        "  def setup(self):\n",
        "    nlayers = len(self.features)\n",
        "    layers = []\n",
        "    for i in range(nlayers):\n",
        "      W = self.params_init[f'W{i}'];\n",
        "      b = self.params_init[f'b{i}']; \n",
        "      weights_init = make_const_init(W)\n",
        "      bias_init = make_const_init(b)\n",
        "      layer = nn.Dense(self.features[i], kernel_init=weights_init, bias_init=bias_init)\n",
        "      layers.append(layer)\n",
        "    self.layers = layers\n",
        "\n",
        "  def __call__(self, inputs):\n",
        "    x = inputs\n",
        "    for i, lyr in enumerate(self.layers):\n",
        "      x = lyr(x)\n",
        "      if i != len(self.layers) - 1:\n",
        "        x = nn.relu(x)\n",
        "    return x\n",
        "\n",
        "params_init = params0\n",
        "model = MLP_init(layer_sizes, params_init)\n",
        "print(model)\n",
        "\n",
        "params = model.init(key, x)\n",
        "print(params)\n",
        "\n",
        "y = model.apply(params, x)\n",
        "print(y)\n",
        "\n",
        "assert np.allclose(y, y0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MLP_init(\n",
            "    # attributes\n",
            "    features = [3, 1]\n",
            "    params_init = {'W0': DeviceArray([[-1.83021, 1.18417, 0.06777],\n",
            "                 [0.34588, 0.37858, -0.65318],\n",
            "                 [0.18976, 0.45157, -0.33964]], dtype=float32), 'b0': DeviceArray([0.00000, 0.00000, 0.00000], dtype=float32), 'W1': DeviceArray([[-1.74905],\n",
            "                 [1.83313],\n",
            "                 [-0.23808]], dtype=float32), 'b1': DeviceArray([0.00000], dtype=float32)}\n",
            ")\n",
            "FrozenDict({\n",
            "    params: {\n",
            "        layers_0: {\n",
            "            kernel: DeviceArray([[-1.83021, 1.18417, 0.06777],\n",
            "                         [0.34588, 0.37858, -0.65318],\n",
            "                         [0.18976, 0.45157, -0.33964]], dtype=float32),\n",
            "            bias: DeviceArray([0.00000, 0.00000, 0.00000], dtype=float32),\n",
            "        },\n",
            "        layers_1: {\n",
            "            kernel: DeviceArray([[-1.74905],\n",
            "                         [1.83313],\n",
            "                         [-0.23808]], dtype=float32),\n",
            "            bias: DeviceArray([0.00000], dtype=float32),\n",
            "        },\n",
            "    },\n",
            "})\n",
            "[[-0.09538]\n",
            " [2.78382]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63YD32Ty-iiD"
      },
      "source": [
        "# Nested modules\n",
        "\n",
        "We can embed an MLP inside an MLP."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwjaFly4-qeZ",
        "outputId": "597cf45a-e98b-4e7b-f08c-7946db5e2dfe"
      },
      "source": [
        "class MLP_nested(nn.Module):\n",
        "    features_nested: Sequence[int]\n",
        "    features_output: int\n",
        "\n",
        "    def setup(self):\n",
        "      self.mlp = MLP(self.features_nested)\n",
        "      self.output_layer = nn.Dense(self.features_output)\n",
        "\n",
        "    def __call__(self, x):\n",
        "      return self.output_layer(nn.relu(self.mlp(x)))\n",
        "\n",
        "\n",
        "model = MLP_nested([3,4], 1)\n",
        "print(model)\n",
        "\n",
        "params = model.init(key, x)\n",
        "print(params)\n",
        "\n",
        "y = model.apply(params, x)\n",
        "print(y)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MLP_nested(\n",
            "    # attributes\n",
            "    features_nested = [3, 4]\n",
            "    features_output = 1\n",
            ")\n",
            "FrozenDict({\n",
            "    params: {\n",
            "        mlp: {\n",
            "            Dense_0: {\n",
            "                kernel: DeviceArray([[-0.63572, 0.33321, 0.62651],\n",
            "                             [-0.17779, 0.22005, 0.74431],\n",
            "                             [-0.97726, -0.47203, 0.43659]], dtype=float32),\n",
            "                bias: DeviceArray([0.00000, 0.00000, 0.00000], dtype=float32),\n",
            "            },\n",
            "            Dense_1: {\n",
            "                kernel: DeviceArray([[0.22112, 0.87654, 0.22310, -1.22264],\n",
            "                             [-0.32240, 0.94956, -0.04660, 0.39572],\n",
            "                             [-0.03547, -0.75176, 0.35311, 0.59318]], dtype=float32),\n",
            "                bias: DeviceArray([0.00000, 0.00000, 0.00000, 0.00000], dtype=float32),\n",
            "            },\n",
            "        },\n",
            "        output_layer: {\n",
            "            kernel: DeviceArray([[0.08588],\n",
            "                         [0.64215],\n",
            "                         [-0.46796],\n",
            "                         [-0.10338]], dtype=float32),\n",
            "            bias: DeviceArray([0.00000], dtype=float32),\n",
            "        },\n",
            "    },\n",
            "})\n",
            "[[0.00000]\n",
            " [-0.21694]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuL88xCopD3J"
      },
      "source": [
        "We can also use the compact notation. The resulting parameters have slightly different names, and random initial values. (They can be made the same, of course.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeY9Jx9NreXm",
        "outputId": "65559e99-0fce-4df6-9e07-be24302c8c6c"
      },
      "source": [
        "class MLP_nested(nn.Module):\n",
        "    features_nested: Sequence[int]\n",
        "    features_output: int\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "      mlp = MLP(self.features_nested, name=\"my_nested_MLP\")\n",
        "      dense = nn.Dense(self.features_output)\n",
        "      return dense(nn.relu(mlp(x)))\n",
        "\n",
        "model = MLP_nested([3,4], 1)\n",
        "\n",
        "params = model.init(key, x)\n",
        "print(params)\n",
        "\n",
        "y = model.apply(params, x)\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FrozenDict({\n",
            "    params: {\n",
            "        my_nested_MLP: {\n",
            "            Dense_0: {\n",
            "                kernel: DeviceArray([[-0.22084, -0.04334, -0.92993],\n",
            "                             [-0.42234, 0.45470, -0.09518],\n",
            "                             [0.86117, 0.10712, 0.50940]], dtype=float32),\n",
            "                bias: DeviceArray([0.00000, 0.00000, 0.00000], dtype=float32),\n",
            "            },\n",
            "            Dense_1: {\n",
            "                kernel: DeviceArray([[-0.17214, -0.46445, 0.01060, -0.84916],\n",
            "                             [0.56824, 0.08029, 0.90197, -0.04891],\n",
            "                             [0.65304, 0.74382, 0.71842, 0.96025]], dtype=float32),\n",
            "                bias: DeviceArray([0.00000, 0.00000, 0.00000, 0.00000], dtype=float32),\n",
            "            },\n",
            "        },\n",
            "        Dense_0: {\n",
            "            kernel: DeviceArray([[0.88803],\n",
            "                         [-0.85732],\n",
            "                         [0.13939],\n",
            "                         [0.10748]], dtype=float32),\n",
            "            bias: DeviceArray([0.00000], dtype=float32),\n",
            "        },\n",
            "    },\n",
            "})\n",
            "[[0.02943]\n",
            " [0.02495]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rf8avaA_nGJ1"
      },
      "source": [
        "# Creating modules with parameters\n",
        "\n",
        "Now we illustrate how to create a module with its own parameters, instead of relying on composing built-in primitives. As an example, we write our own dense layer class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUJ98XpSnS8F",
        "outputId": "0238844e-3954-4591-943c-8072597f61d1"
      },
      "source": [
        "class SimpleDense(nn.Module):\n",
        "  features: int # num output features for this layer\n",
        "  kernel_init: Callable = nn.initializers.lecun_normal()\n",
        "  bias_init: Callable = nn.initializers.zeros\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, inputs):\n",
        "    features_in = inputs.shape[-1] # infer shape from input\n",
        "    features_out = self.features\n",
        "    kernel = self.param('kernel', self.kernel_init, (features_in, features_out))\n",
        "    bias = self.param('bias', self.bias_init, (features_out,))\n",
        "    outputs = jnp.dot(inputs, kernel) + bias\n",
        "    return outputs\n",
        "\n",
        "\n",
        "model = SimpleDense(features=3)\n",
        "print(model)\n",
        "\n",
        "params = model.init(key, x)\n",
        "print(params)\n",
        "\n",
        "y = model.apply(params, x)\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SimpleDense(\n",
            "    # attributes\n",
            "    features = 3\n",
            "    kernel_init = init\n",
            "    bias_init = zeros\n",
            ")\n",
            "FrozenDict({\n",
            "    params: {\n",
            "        kernel: DeviceArray([[0.32718, 0.05599, 0.17998],\n",
            "                     [-0.12295, 0.70712, 0.28972],\n",
            "                     [0.13731, -0.02853, -0.62830]], dtype=float32),\n",
            "        bias: DeviceArray([0.00000, 0.00000, 0.00000], dtype=float32),\n",
            "    },\n",
            "})\n",
            "[[0.30842 -0.91549 -0.74603]\n",
            " [0.36248 0.24616 0.36943]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJpBh933-GTW"
      },
      "source": [
        "# Stochastic layers\n",
        "\n",
        "Some layers may need a source of randomness. If so, we must pass them a PRNG in the `init` and `apply` functions, in addition to the PRNG used for parameter initialization. We illustrate this below using dropout. We construct two versions, one which is stochastic (for training), and one which is deterministic (for evaluation). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMSpLucO-Yfj",
        "outputId": "9726f1f3-ca25-4946-f9da-29692b1b034c"
      },
      "source": [
        "class Block(nn.Module):\n",
        "  features: int\n",
        "  training: bool\n",
        "  @nn.compact\n",
        "  def __call__(self, inputs):\n",
        "    x = nn.Dense(self.features)(inputs)\n",
        "    x = nn.Dropout(rate=0.5)(x, deterministic=not self.training)\n",
        "    return x\n",
        "\n",
        "N = 1; D = 2;\n",
        "x = random.uniform(key, (N,D))\n",
        "\n",
        "model = Block(features=3, training=True)\n",
        "key = random.PRNGKey(0)\n",
        "variables = model.init({'params': key, 'dropout': key}, x)\n",
        "#variables = model.init(key, x) # cannot share the rng\n",
        "print('variables', variables)\n",
        "\n",
        "# Apply stochastic model\n",
        "for i in range(2):\n",
        "  key, subkey = random.split(key)\n",
        "  y = model.apply(variables, x, rngs={'dropout': subkey})\n",
        "  print(f'train output {i}, ', y)\n",
        "\n",
        "# Now make a deterministic version\n",
        "eval_model = Block(features=3, training=False)\n",
        "key = random.PRNGKey(0)\n",
        "#variables = eval_model.init({'params': key, 'dropout': key}, x)\n",
        "for i in range(2):\n",
        "  key, subkey = random.split(key)\n",
        "  y = eval_model.apply(variables, x, rngs={'dropout': subkey})\n",
        "  print(f'eval output {i}, ', y)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "variables FrozenDict({\n",
            "    params: {\n",
            "        Dense_0: {\n",
            "            kernel: DeviceArray([[0.99988, -0.14086, -0.99796],\n",
            "                         [1.46673, 0.59637, 0.38263]], dtype=float32),\n",
            "            bias: DeviceArray([0.00000, 0.00000, 0.00000], dtype=float32),\n",
            "        },\n",
            "    },\n",
            "})\n",
            "train output 0,  [[0.00000 1.05814 0.00000]]\n",
            "train output 1,  [[3.12202 1.05814 0.32862]]\n",
            "eval output 0,  [[1.56101 0.52907 0.16431]]\n",
            "eval output 1,  [[1.56101 0.52907 0.16431]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHieB2aAumdg"
      },
      "source": [
        "# Mutable variables\n",
        "\n",
        "In addition to parameters, linen modules can contain other kinds of variables, which may be mutable as we illustrate below.\n",
        "Indeed, parameters are just a special case of variable.\n",
        "In particular, this line\n",
        "```\n",
        "p = self.param('param_name', init_fn, shape, dtype)\n",
        "```\n",
        "is a convenient shorthand for this:\n",
        "```\n",
        "p = self.variable('params', 'param_name', lambda s, d: init_fn(self.make_rng('params'), s, d), shape, dtype).value\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAQxx2Tu8xln"
      },
      "source": [
        "## Example: counter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeGNa8zaut41",
        "outputId": "eb8923e0-ed62-46f9-f11b-80dec053e31a"
      },
      "source": [
        "class Counter(nn.Module):\n",
        "  @nn.compact\n",
        "  def __call__(self):\n",
        "    # variable(collection, name, init_fn, *init_args)\n",
        "    counter1 = self.variable('counter', 'count1', lambda: jnp.zeros((), jnp.int32))\n",
        "    counter2 = self.variable('counter', 'count2', lambda: jnp.zeros((), jnp.int32))\n",
        "    is_initialized = self.has_variable('counter', 'count1')\n",
        "    if is_initialized:\n",
        "      counter1.value += 1\n",
        "      counter2.value += 2\n",
        "    return counter1.value, counter2.value\n",
        "\n",
        "\n",
        "model = Counter()\n",
        "print(model)\n",
        "\n",
        "init_variables = model.init(key) # calls the `call` method\n",
        "print('initialized variables:\\n', init_variables)\n",
        "counter = init_variables['counter']['count1']\n",
        "print('counter 1 value', counter)\n",
        "\n",
        "y, mutated_variables = model.apply(init_variables, mutable=['counter'])\n",
        "print('mutated variables:\\n', mutated_variables)\n",
        "print('output:\\n', y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter()\n",
            "initialized variables:\n",
            " FrozenDict({\n",
            "    counter: {\n",
            "        count1: DeviceArray(1, dtype=int32),\n",
            "        count2: DeviceArray(2, dtype=int32),\n",
            "    },\n",
            "})\n",
            "counter 1 value 1\n",
            "mutated variables:\n",
            " FrozenDict({\n",
            "    counter: {\n",
            "        count1: DeviceArray(2, dtype=int32),\n",
            "        count2: DeviceArray(4, dtype=int32),\n",
            "    },\n",
            "})\n",
            "output:\n",
            " (DeviceArray(2, dtype=int32), DeviceArray(4, dtype=int32))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IaC2RT1v65t"
      },
      "source": [
        "## Combining mutable variables and immutable parameters\n",
        "\n",
        "We can combine mutable variables with immutable parameters.\n",
        "As an example, consider a simplified version of batch normalization, which \n",
        " computes the running mean of its inputs, and adds an optimzable offset (bias) term. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXP19telv_Y_"
      },
      "source": [
        "class BiasAdderWithRunningMean(nn.Module):\n",
        "  decay: float = 0.99\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    is_initialized = self.has_variable('params', 'bias')\n",
        "\n",
        "    # variable(collection, name, init_fn, *init_args)\n",
        "    ra_mean = self.variable('batch_stats', 'mean', lambda s: jnp.zeros(s), x.shape[1:])\n",
        "\n",
        "    dummy_mutable = self.variable('mutables', 'dummy', lambda s: 42, 0)\n",
        "\n",
        "    # param(name, init_fn, *init_args)\n",
        "    bias = self.param('bias', lambda rng, shape: jnp.ones(shape), x.shape[1:]) \n",
        "\n",
        "    if is_initialized:\n",
        "      ra_mean.value = self.decay * ra_mean.value + (1.0 - self.decay) * jnp.mean(x, axis=0, keepdims=True)\n",
        "\n",
        "    return x - ra_mean.value + bias\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_WsMGY8xA_x"
      },
      "source": [
        "\n",
        "The intial variables are:\n",
        "params = (bias=1), batch_stats=(mean=0)\n",
        "\n",
        "If we pass in x=ones(N,D), the  running average becomes\n",
        "$$\n",
        "0.99*0 + (1-0.99)*1 = 0.01\n",
        "$$\n",
        "and the output becomes\n",
        "$$\n",
        "1 - 0.01 + 1 = 1.99\n",
        "$$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvXKCE8yxiTu",
        "outputId": "4ddb1117-32a0-481d-d8bb-876010d0e821"
      },
      "source": [
        "key = random.PRNGKey(0)\n",
        "N = 2\n",
        "D = 5\n",
        "x = jnp.ones((N,D))\n",
        "model = BiasAdderWithRunningMean()\n",
        "\n",
        "variables = model.init(key, x)\n",
        "print('initial variables:\\n', variables)\n",
        "nonstats, stats = variables.pop('batch_stats')\n",
        "print('nonstats', nonstats)\n",
        "print('stats', stats)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "initial variables:\n",
            " FrozenDict({\n",
            "    batch_stats: {\n",
            "        mean: DeviceArray([0.00000, 0.00000, 0.00000, 0.00000, 0.00000], dtype=float32),\n",
            "    },\n",
            "    mutables: {\n",
            "        dummy: 42,\n",
            "    },\n",
            "    params: {\n",
            "        bias: DeviceArray([1.00000, 1.00000, 1.00000, 1.00000, 1.00000], dtype=float32),\n",
            "    },\n",
            "})\n",
            "nonstats FrozenDict({\n",
            "    mutables: {\n",
            "        dummy: 42,\n",
            "    },\n",
            "    params: {\n",
            "        bias: DeviceArray([1.00000, 1.00000, 1.00000, 1.00000, 1.00000], dtype=float32),\n",
            "    },\n",
            "})\n",
            "stats FrozenDict({\n",
            "    mean: DeviceArray([0.00000, 0.00000, 0.00000, 0.00000, 0.00000], dtype=float32),\n",
            "})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ytr2_w9U12PT",
        "outputId": "30555a51-9b09-4ef2-8222-98c9b52e4a47"
      },
      "source": [
        "y, mutables = model.apply(variables, x, mutable=['batch_stats'])\n",
        "print('output', y)\n",
        "print('mutables', mutables)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "output [[1.99000 1.99000 1.99000 1.99000 1.99000]\n",
            " [1.99000 1.99000 1.99000 1.99000 1.99000]]\n",
            "mutables FrozenDict({\n",
            "    batch_stats: {\n",
            "        mean: DeviceArray([[0.01000, 0.01000, 0.01000, 0.01000, 0.01000]], dtype=float32),\n",
            "    },\n",
            "})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1g2GW3f3B-Z"
      },
      "source": [
        "To call the function with the updated batch stats, we have to stitch together the new mutated state with the old state, as shown below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpBb21A72Bdj",
        "outputId": "d5ce7521-90c4-48a0-a180-4f02be5fe5f8"
      },
      "source": [
        "\n",
        "variables = unfreeze(nonstats)\n",
        "print(variables)\n",
        "variables['batch_stats'] = mutables['batch_stats']\n",
        "variables = freeze(variables)\n",
        "print(variables)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'mutables': {'dummy': 42}, 'params': {'bias': DeviceArray([1.00000, 1.00000, 1.00000, 1.00000, 1.00000], dtype=float32)}}\n",
            "FrozenDict({\n",
            "    mutables: {\n",
            "        dummy: 42,\n",
            "    },\n",
            "    params: {\n",
            "        bias: DeviceArray([1.00000, 1.00000, 1.00000, 1.00000, 1.00000], dtype=float32),\n",
            "    },\n",
            "    batch_stats: {\n",
            "        mean: DeviceArray([[0.01000, 0.01000, 0.01000, 0.01000, 0.01000]], dtype=float32),\n",
            "    },\n",
            "})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sa7nH74Y5-Lg"
      },
      "source": [
        "If we pass in x=2*ones(N,D), the running average gets updated to\n",
        "$$\n",
        "0.99 * 0.01 + (1-0.99) * 2.0 = 0.0299\n",
        "$$\n",
        "and the output becomes\n",
        "$$\n",
        "2- 0.0299 + 1 = 2.9701\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2dF2si51QN5",
        "outputId": "6177ee1c-41b7-40e6-d954-d0c1c85b0180"
      },
      "source": [
        "\n",
        "x = 2*jnp.ones((N,D))\n",
        "y, mutables = model.apply(variables, x, mutable=['batch_stats'])\n",
        "print('output', y)\n",
        "print('batch_stats', mutables)\n",
        "\n",
        "assert np.allclose(y, 2.9701)\n",
        "assert np.allclose(mutables['batch_stats']['mean'], 0.0299)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "output [[2.97010 2.97010 2.97010 2.97010 2.97010]\n",
            " [2.97010 2.97010 2.97010 2.97010 2.97010]]\n",
            "batch_stats FrozenDict({\n",
            "    batch_stats: {\n",
            "        mean: DeviceArray([[0.02990, 0.02990, 0.02990, 0.02990, 0.02990]], dtype=float32),\n",
            "    },\n",
            "})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnBmgGxOoPKU"
      },
      "source": [
        "# Optimization\n",
        "\n",
        "Flax has several built-in (first-order) optimizers, as we illustrate below on a random linear function. (Note that we can also fit a model defined in flax using some other kind of optimizer, such as that provided by the [optax library](https://github.com/deepmind/optax).)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTHgj_pMra3H",
        "outputId": "d142c2bb-c725-47e6-8cba-5b55f9f16b48"
      },
      "source": [
        "D = 5\n",
        "key = jax.random.PRNGKey(0)\n",
        "params = {'w': jax.random.normal(key, (D,))}\n",
        "print(params)\n",
        "\n",
        "x = jax.random.normal(key, (D,))\n",
        "\n",
        "def loss(params):\n",
        "  w = params['w']\n",
        "  return jnp.dot(x, w)\n",
        "\n",
        "loss_grad_fn = jax.value_and_grad(loss)\n",
        "v, g = loss_grad_fn(params)\n",
        "print(v)\n",
        "print(g)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'w': DeviceArray([0.18784, -1.28334, -0.27109, 1.24906, 0.24447], dtype=float32)}\n",
            "3.375659\n",
            "{'w': DeviceArray([0.18784, -1.28334, -0.27109, 1.24906, 0.24447], dtype=float32)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KdmBHa8oWFY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3695c5f5-339c-4e27-d80e-30d100ddae66"
      },
      "source": [
        "from flax import optim\n",
        "optimizer_def = optim.Momentum(learning_rate=0.1, beta=0.9)\n",
        "print(optimizer_def)\n",
        "\n",
        "optimizer = optimizer_def.create(params) \n",
        "print(optimizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<flax.optim.momentum.Momentum object at 0x7fbd09abceb8>\n",
            "Optimizer(optimizer_def=<flax.optim.momentum.Momentum object at 0x7fbd09abceb8>, state=OptimizerState(step=DeviceArray(0, dtype=int32), param_states={'w': _MomentumParamState(momentum=DeviceArray([0.00000, 0.00000, 0.00000, 0.00000, 0.00000], dtype=float32))}), target={'w': DeviceArray([0.18784, -1.28334, -0.27109, 1.24906, 0.24447], dtype=float32)})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JpgauX_ox_w",
        "outputId": "c649c61c-93ba-41a0-a834-2c254a53a243"
      },
      "source": [
        "for i in range(10):\n",
        "  params = optimizer.target\n",
        "  loss_val, grad = loss_grad_fn(params)\n",
        "  optimizer = optimizer.apply_gradient(grad)\n",
        "  params = optimizer.target\n",
        "  print('step {}, loss {:0.3f}, params {}'.format(i, loss_val, params))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step 0, loss -10.593, params {'w': DeviceArray([-0.71837, 4.90788, 1.03673, -4.77677, -0.93493], dtype=float32)}\n",
            "step 1, loss -12.910, params {'w': DeviceArray([-0.85316, 5.82877, 1.23126, -5.67306, -1.11035], dtype=float32)}\n",
            "step 2, loss -15.332, params {'w': DeviceArray([-0.99326, 6.78590, 1.43345, -6.60462, -1.29268], dtype=float32)}\n",
            "step 3, loss -17.849, params {'w': DeviceArray([-1.13813, 7.77566, 1.64252, -7.56794, -1.48122], dtype=float32)}\n",
            "step 4, loss -20.453, params {'w': DeviceArray([-1.28730, 8.79477, 1.85780, -8.55983, -1.67536], dtype=float32)}\n",
            "step 5, loss -23.133, params {'w': DeviceArray([-1.44033, 9.84031, 2.07866, -9.57743, -1.87453], dtype=float32)}\n",
            "step 6, loss -25.884, params {'w': DeviceArray([-1.59685, 10.90963, 2.30454, -10.61818, -2.07823], dtype=float32)}\n",
            "step 7, loss -28.696, params {'w': DeviceArray([-1.75650, 12.00035, 2.53494, -11.67977, -2.28600], dtype=float32)}\n",
            "step 8, loss -31.565, params {'w': DeviceArray([-1.91897, 13.11033, 2.76941, -12.76010, -2.49745], dtype=float32)}\n",
            "step 9, loss -34.485, params {'w': DeviceArray([-2.08397, 14.23764, 3.00754, -13.85730, -2.71220], dtype=float32)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ITTDWT2ECxC"
      },
      "source": [
        "# Worked example\n",
        "\n",
        "We demonstrate how to fit a shallow MLP to MNIST using Flax."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_xSZi3v03pC"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264,
          "referenced_widgets": [
            "d7f80b4703ea49f68156b9a2d2340692",
            "e0a4d052d22a4d47b516b95c8ac3acc5",
            "a157229195ab41d18d8d937d8750f800",
            "b8db32e87d7140ae93665d859f71e3c5",
            "f41bf22b33b3424c9acaf2f2fe5102a0",
            "e6d5accc6bfd406a9364bf2aff28af70",
            "fde4e6bb53824791801dbbe6e804019e",
            "f1525b505268487a996471820838d64b"
          ]
        },
        "id": "klFERmagmBlr",
        "outputId": "5b0458cc-a5d4-4feb-ca81-f585f1ded17e"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "def process_tf(batch):\n",
        "  image = batch['image']\n",
        "  X = tf.cast(image, tf.float32) / 255. # TF yuck\n",
        "  X = tf.reshape(X, [-1, 28*28*1]) # flatten # gives (bs,1,784) - why the 1?\n",
        "  d = {'X': X,\n",
        "        'y': batch['label']}\n",
        "  return d\n",
        "\n",
        "def load_dataset_mnist(split: tfds.Split, batch_size: int) -> Iterator[Batch]:\n",
        "  ds, ds_info = tfds.load(\"mnist\", split=split, with_info=True)\n",
        "  #ds = ds.map(process_tf)\n",
        "  # For true randomness, we set the shuffle buffer to the full dataset size.\n",
        "  ds = ds.shuffle(ds_info.splits[split].num_examples)\n",
        "  ds = ds.batch(batch_size)\n",
        "  ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  ds = ds.repeat()\n",
        "  ds = tfds.as_numpy(ds)\n",
        "  return iter(ds)\n",
        "\n",
        "\n",
        "batch_size = 30\n",
        "train_ds = load_dataset_mnist(tfds.Split.TRAIN, batch_size)\n",
        "batch = next(train_ds)\n",
        "print(batch['image'].shape)\n",
        "print(batch['label'].shape)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mDownloading and preparing dataset mnist/3.0.1 (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to /root/tensorflow_datasets/mnist/3.0.1...\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Dataset mnist is hosted on GCS. It will automatically be downloaded to your\n",
            "local data directory. If you'd instead prefer to read directly from our public\n",
            "GCS bucket (recommended if you're running on GCP), you can instead pass\n",
            "`try_gcs=True` to `tfds.load` or set `data_dir=gs://tfds-data/datasets`.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d7f80b4703ea49f68156b9a2d2340692",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Dl Completed...', max=4.0, style=ProgressStyle(descriptioâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1mDataset mnist downloaded and prepared to /root/tensorflow_datasets/mnist/3.0.1. Subsequent calls will reuse this data.\u001b[0m\n",
            "(30, 28, 28, 1)\n",
            "(30,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-DNEPsXtWVO",
        "outputId": "7410b398-f5c9-406a-cf15-c77ce1242e94"
      },
      "source": [
        "def preprocess_batch(batch, rng): #numpy version\n",
        "  X = batch['image']\n",
        "  y = batch['label']\n",
        "  X = np.reshape(X, (X.shape[0], -1))\n",
        "  X = X.astype(np.float32)/255\n",
        "  return {'X': X, 'y': y}\n",
        "\n",
        "rng = jax.random.PRNGKey(0)\n",
        "batch = next(train_ds)\n",
        "batch2 = preprocess_batch(batch, rng)\n",
        "print(batch2['X'].shape)\n",
        "print(batch2['y'].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(30, 784)\n",
            "(30,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLiWUSjR05BQ"
      },
      "source": [
        "## Model\n",
        "\n",
        "We fit a one layer MLP."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLwAwqd4Nzvy"
      },
      "source": [
        "class Model(nn.Module):\n",
        "  nhidden: int\n",
        "  nclasses: int\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    if self.nhidden > 0:\n",
        "      x = nn.Dense(self.nhidden)(x)\n",
        "      x = nn.relu(x)\n",
        "    x = nn.Dense(self.nclasses)(x)\n",
        "    x = nn.log_softmax(x)\n",
        "    return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JsVFGfU628j"
      },
      "source": [
        "## Generic fit function\n",
        "\n",
        "Code is from https://github.com/probml/pyprobml/blob/master/scripts/fit_flax.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzSEEjUhoLAA",
        "outputId": "03508994-89a1-4d61-81a8-06976e70598e"
      },
      "source": [
        "# Book code\n",
        "!rm -rf pyprobml # Run this first if the pyprobml directory already exists\n",
        "!git clone https://github.com/probml/pyprobml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'pyprobml'...\n",
            "remote: Enumerating objects: 66, done.\u001b[K\n",
            "remote: Counting objects: 100% (66/66), done.\u001b[K\n",
            "remote: Compressing objects: 100% (57/57), done.\u001b[K\n",
            "remote: Total 5493 (delta 32), reused 20 (delta 9), pack-reused 5427\u001b[K\n",
            "Receiving objects: 100% (5493/5493), 197.46 MiB | 17.19 MiB/s, done.\n",
            "Resolving deltas: 100% (3087/3087), done.\n",
            "Checking out files: 100% (473/473), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViKR05jUoRxK",
        "outputId": "5bb5b754-6eae-475d-cdf9-2fd76669df62"
      },
      "source": [
        "#os.chdir('pyprobml/scripts')\n",
        "import pyprobml.scripts.fit_flax as ff\n",
        "ff.fit_model_test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train step: 0, loss: 1.7738, accuracy: 0.00\n",
            "train step: 1, loss: 1.5240, accuracy: 0.33\n",
            "test passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 825
        },
        "id": "3gup-OrNpLXr",
        "outputId": "52b43c33-85bb-444b-ca69-0400717ef1e4"
      },
      "source": [
        "make_optimizer = optim.Momentum(learning_rate=0.1, beta=0.9)\n",
        "\n",
        "model = Model(nhidden = 128, nclasses=10) \n",
        "\n",
        "batch_size = 32 \n",
        "train_ds = load_dataset_mnist(tfds.Split.TRAIN, batch_size)\n",
        "test_ds = load_dataset_mnist(tfds.Split.TEST, batch_size)\n",
        "\n",
        "rng = jax.random.PRNGKey(0)\n",
        "num_steps = 200\n",
        "\n",
        "params, history =  ff.fit_model(model, train_ds, test_ds,  rng,\n",
        "        num_steps, make_optimizer,\n",
        "        ff.train_batch, ff.eval_batch, \n",
        "        preprocess_batch, preprocess_batch,\n",
        "        print_every=20, eval_every=10)\n",
        "  \n",
        "display(history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train step: 0, loss: 2.3136, accuracy: 0.09\n",
            "train step: 20, loss: 0.8159, accuracy: 0.66\n",
            "train step: 40, loss: 0.9157, accuracy: 0.84\n",
            "train step: 60, loss: 0.2726, accuracy: 0.88\n",
            "train step: 80, loss: 0.9786, accuracy: 0.66\n",
            "train step: 100, loss: 0.4131, accuracy: 0.84\n",
            "train step: 120, loss: 0.5677, accuracy: 0.88\n",
            "train step: 140, loss: 0.3517, accuracy: 0.94\n",
            "train step: 160, loss: 0.0641, accuracy: 0.97\n",
            "train step: 180, loss: 0.3618, accuracy: 0.88\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train_loss</th>\n",
              "      <th>train_accuracy</th>\n",
              "      <th>test_loss</th>\n",
              "      <th>test_accuracy</th>\n",
              "      <th>step</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.31363</td>\n",
              "      <td>0.09375</td>\n",
              "      <td>2.2760365</td>\n",
              "      <td>0.09375</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.3751764</td>\n",
              "      <td>0.59375</td>\n",
              "      <td>0.95285213</td>\n",
              "      <td>0.625</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.8158629</td>\n",
              "      <td>0.65625</td>\n",
              "      <td>0.8337441</td>\n",
              "      <td>0.78125</td>\n",
              "      <td>20.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.66259676</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.64875305</td>\n",
              "      <td>0.84375</td>\n",
              "      <td>30.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.9157165</td>\n",
              "      <td>0.84375</td>\n",
              "      <td>0.3847033</td>\n",
              "      <td>0.90625</td>\n",
              "      <td>40.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1.1061184</td>\n",
              "      <td>0.71875</td>\n",
              "      <td>0.5960076</td>\n",
              "      <td>0.875</td>\n",
              "      <td>50.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.27263454</td>\n",
              "      <td>0.875</td>\n",
              "      <td>0.65843666</td>\n",
              "      <td>0.78125</td>\n",
              "      <td>60.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.87926686</td>\n",
              "      <td>0.71875</td>\n",
              "      <td>0.38330597</td>\n",
              "      <td>0.84375</td>\n",
              "      <td>70.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.978621</td>\n",
              "      <td>0.65625</td>\n",
              "      <td>0.5121664</td>\n",
              "      <td>0.75</td>\n",
              "      <td>80.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.41085285</td>\n",
              "      <td>0.90625</td>\n",
              "      <td>0.47765964</td>\n",
              "      <td>0.84375</td>\n",
              "      <td>90.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.41308284</td>\n",
              "      <td>0.84375</td>\n",
              "      <td>0.44917065</td>\n",
              "      <td>0.84375</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.35392252</td>\n",
              "      <td>0.875</td>\n",
              "      <td>0.13358633</td>\n",
              "      <td>0.96875</td>\n",
              "      <td>110.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.567686</td>\n",
              "      <td>0.875</td>\n",
              "      <td>0.46620435</td>\n",
              "      <td>0.75</td>\n",
              "      <td>120.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.23645164</td>\n",
              "      <td>0.90625</td>\n",
              "      <td>0.5451975</td>\n",
              "      <td>0.875</td>\n",
              "      <td>130.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.3516686</td>\n",
              "      <td>0.9375</td>\n",
              "      <td>0.2801758</td>\n",
              "      <td>0.875</td>\n",
              "      <td>140.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.43907472</td>\n",
              "      <td>0.84375</td>\n",
              "      <td>0.494274</td>\n",
              "      <td>0.84375</td>\n",
              "      <td>150.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.06414427</td>\n",
              "      <td>0.96875</td>\n",
              "      <td>0.36549434</td>\n",
              "      <td>0.90625</td>\n",
              "      <td>160.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.40481865</td>\n",
              "      <td>0.875</td>\n",
              "      <td>0.15323508</td>\n",
              "      <td>0.96875</td>\n",
              "      <td>170.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.36179838</td>\n",
              "      <td>0.875</td>\n",
              "      <td>0.18805325</td>\n",
              "      <td>0.96875</td>\n",
              "      <td>180.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.49327964</td>\n",
              "      <td>0.84375</td>\n",
              "      <td>0.341471</td>\n",
              "      <td>0.875</td>\n",
              "      <td>190.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    train_loss train_accuracy   test_loss test_accuracy   step\n",
              "0      2.31363        0.09375   2.2760365       0.09375    0.0\n",
              "1    1.3751764        0.59375  0.95285213         0.625   10.0\n",
              "2    0.8158629        0.65625   0.8337441       0.78125   20.0\n",
              "3   0.66259676           0.75  0.64875305       0.84375   30.0\n",
              "4    0.9157165        0.84375   0.3847033       0.90625   40.0\n",
              "5    1.1061184        0.71875   0.5960076         0.875   50.0\n",
              "6   0.27263454          0.875  0.65843666       0.78125   60.0\n",
              "7   0.87926686        0.71875  0.38330597       0.84375   70.0\n",
              "8     0.978621        0.65625   0.5121664          0.75   80.0\n",
              "9   0.41085285        0.90625  0.47765964       0.84375   90.0\n",
              "10  0.41308284        0.84375  0.44917065       0.84375  100.0\n",
              "11  0.35392252          0.875  0.13358633       0.96875  110.0\n",
              "12    0.567686          0.875  0.46620435          0.75  120.0\n",
              "13  0.23645164        0.90625   0.5451975         0.875  130.0\n",
              "14   0.3516686         0.9375   0.2801758         0.875  140.0\n",
              "15  0.43907472        0.84375    0.494274       0.84375  150.0\n",
              "16  0.06414427        0.96875  0.36549434       0.90625  160.0\n",
              "17  0.40481865          0.875  0.15323508       0.96875  170.0\n",
              "18  0.36179838          0.875  0.18805325       0.96875  180.0\n",
              "19  0.49327964        0.84375    0.341471         0.875  190.0"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "RWv2Sspl8EAN",
        "outputId": "2c19671d-f5a9-4df7-aa93-0a787b688aa2"
      },
      "source": [
        "plt.figure()\n",
        "plt.plot(history['step'], history['test_accuracy'], 'o-', label='test accuracy')\n",
        "plt.xlabel('num. minibatches')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWe69Z51Q3Kz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
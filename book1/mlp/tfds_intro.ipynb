{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tfds_intro.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/probml/pyprobml/blob/master/book1/mlp/tfds_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xX4GSX3Fwt1S"
      },
      "source": [
        "# Introduction to tensorflow datasets\n",
        "\n",
        "[TFDS](https://www.tensorflow.org/datasets) is a handy way to handle large datasets as a stream of batches. It can be used by tensorflow and JAX code. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7H7qrB8xT8J"
      },
      "source": [
        "# Standard Python libraries\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL\n",
        "import imageio\n",
        "\n",
        "from IPython import display\n",
        "\n",
        "import sklearn\n",
        "\n",
        "import seaborn as sns;\n",
        "sns.set(style=\"ticks\", color_codes=True)\n",
        "\n",
        "import pandas as pd\n",
        "pd.set_option('precision', 2) # 2 decimal places\n",
        "pd.set_option('display.max_rows', 20)\n",
        "pd.set_option('display.max_columns', 30)\n",
        "pd.set_option('display.width', 100) # wide windows\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYPihPWaXaSv"
      },
      "source": [
        "\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "    IS_COLAB = True\n",
        "except Exception:\n",
        "    IS_COLAB = False\n",
        "\n",
        "# TensorFlow â‰¥2.0 is required\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "print(\"tf version {}\".format(tf.__version__))\n",
        "\n",
        "if not tf.config.list_physical_devices('GPU'):\n",
        "    print(\"No GPU was detected. DNNs can be very slow without a GPU.\")\n",
        "    if IS_COLAB:\n",
        "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkUsOZv3GZjg"
      },
      "source": [
        "# Converting numpy data to stream of batches\n",
        "\n",
        "The functionality similar functionality to PyTorch DataLoader, but natively supports infinite streams via the `repeat` function. Also, all minibatches have the same size (note how we 'wrap around' the dataset).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAieu0vO6Twb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cd8d8b1-4afe-42a7-d484-fbd35e4e707a"
      },
      "source": [
        "N_train = 5\n",
        "D = 4            \n",
        "np.random.seed(0)\n",
        "X = np.random.randn(N_train, D)\n",
        "y = np.random.randn(N_train)\n",
        "print(y)\n",
        "batch_size = 2\n",
        "dataset = tf.data.Dataset.from_tensor_slices({\"X\": X, \"y\": y})\n",
        "batches = dataset.repeat().batch(batch_size)\n",
        "\n",
        "print('batchified version')\n",
        "step = 0\n",
        "num_minibatches = 4\n",
        "for batch in batches:\n",
        "    if step >= num_minibatches:\n",
        "        break\n",
        "    # print(type(batch[\"X\"])) #<class 'tensorflow.python.framework.ops.EagerTensor'\n",
        "    x, y = batch[\"X\"].numpy(), batch[\"y\"].numpy()\n",
        "    print(y)\n",
        "    step = step + 1\n",
        "\n",
        "\n",
        "\n",
        "print('batchified version v2')\n",
        "batch_stream = batches.as_numpy_iterator()\n",
        "for step in range(num_minibatches):\n",
        "  batch = batch_stream.next()\n",
        "  # print(type(batch[\"X\"])) #<class 'numpy.ndarray'>\n",
        "  x, y = batch[\"X\"], batch[\"y\"]\n",
        "  print(y)\n",
        "  step = step + 1"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-2.55298982  0.6536186   0.8644362  -0.74216502  2.26975462]\n",
            "batchified version\n",
            "[-2.55298982  0.6536186 ]\n",
            "[ 0.8644362  -0.74216502]\n",
            "[ 2.26975462 -2.55298982]\n",
            "[0.6536186 0.8644362]\n",
            "batchified version v2\n",
            "[-2.55298982  0.6536186 ]\n",
            "[ 0.8644362  -0.74216502]\n",
            "[ 2.26975462 -2.55298982]\n",
            "[0.6536186 0.8644362]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIeQtCs1X6vh"
      },
      "source": [
        "# Using pre-packaged datasets\n",
        "\n",
        "There are many standard datasets available from https://www.tensorflow.org/datasets. We give some examples below.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMtH7PVMHMw8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f0385e3-fb4d-4045-9e70-a1d207a581f5"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "dataset = tfds.load(name=\"mnist\", split=tfds.Split.TRAIN)\n",
        "\n",
        "batches = dataset.repeat().batch(batch_size)\n",
        "\n",
        "step = 0\n",
        "for batch in batches:\n",
        "    if step >= num_minibatches:\n",
        "        break\n",
        "    X, y = batch['image'], batch['label']\n",
        "    print(type(X))\n",
        "    print(X.shape)\n",
        "    step = step + 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "(2, 28, 28, 1)\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "(2, 28, 28, 1)\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "(2, 28, 28, 1)\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "(2, 28, 28, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}